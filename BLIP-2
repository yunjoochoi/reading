BLIP-2


시각-언어 사전학습 비용이 대규모 모델의 end-to-end 학습으로 인해 점점 더 부담
이 논문에선 사전 학습된 동결(frozen) 이미지 인코더와 대규모 언어 모델(LLM)을 활용하여 시각-언어 사전학습을 부트스트랩하는 BLIP-2라는 범용적이며 효율적인 전략 제시
경량 쿼리 변환기(Q-Former)를 통해 모달리티 간 격차를 해소하고 이는 두 단계의 사전학습 절차로 진행됨
첫 번째 단계에서는 동결 이미지 인코더로부터 시각-언어 표현 학습을 부트스트랩하고, 두 번째 단계에서는 동결 언어 모델로부터 시각-언어 생성 학습을 부트스트랩


종단간 학습은 학습비용이 크므로 프로즌 이미지 인코더와 LLM을 도입 
그러나 LLM은 이미지 본 적이 없기에 프로즌 유지할 경우 시각-언어 정렬이 어려워진다. = 큐 포머 도입 이유
큐 포머란? 동결 이미지 인코더로부터 시각적 특징을 추출하기 위해 학습가능한 쿼리벡터세트를 사용하는 경량 트랜스포머, LLM이 원하는 텍스트 출력을 시각적 특징을 제공해서 도움

공유된 self-attention 레이어로 이미지/텍스트 정보 통합
1. 표현 학습 단계: 이미지-텍스트 대조(ITC), 텍스트 생성(ITG), 매칭(ITM) 목적함수를 결합
2. 생성 학습 단계: Q-Former 출력을 LLM 텍스트 임베딩에 연결
