METHODOLOGY
입력 이미지 분할
입력 이미지를 448×448 크기의 패치로 슬라이딩 윈도우 방식으로 나눔.
각 패치는 다시 14×14 픽셀의 작은 패치로 세분화되어 각각이 토큰으로 처리

로컬 특징 추출
사전학습된 CLIP 기반 Transformer 블록을 활용해, 각 패치 내 토큰들을 개별적으로 처리.
윈도우 간 문맥을 연결하기 위해 Shifted Window Attention을 일정 간격으로 적용.

전역 특징 추출
입력 이미지를 448×448 크기로 리사이즈하여 CLIP에 입력, 글로벌 특징을 추출.
이 전역 특징은 서브이미지에서 얻은 특징과 함께 공유 이미지 리샘플러로 처리되어 언어 도메인과 정렬됨.

토큰 압축 및 정제
Token Resampler를 사용해 토큰 길이를 줄이고, 언어 공간에서의 중복 최소화.

최종 답변 생성
이렇게 처리된 시각 특징들과 입력 질문이 결합되어 LLM이 분석, 원하는 답변을 생성.

### 3.1 Shifted Window Attention

1. **문제 배경**
    - 문서 이해에서는 입력 해상도가 매우 중요.
    - 기존 방법은 **슬라이딩 윈도우**로 이미지를 잘라서 처리 → 자연 장면에는 효과적이지만,
        - 문서 내 **연결된 텍스트가 단절**되는 문제 발생.
        - *텍스트 위치 기반 태스크(예: grounding)**에 불리.
2. **해결 방법: Shifted Window Attention (SWA)**
    - 이미지를 **비중첩(non-overlapping) 윈도우**로 분할.
    - 각 윈도우는 CLIP Transformer 블록으로 처리하되, 기본적으로 **윈도우 간 상호작용 없음**.
    - SWA를 적용하여 윈도우를 **주기적으로 좌상단(top-left) 방향으로 shift** → 새로운 윈도우 구성.
    - **마스킹된 self-attention**을 적용 → 새로운 윈도우 내에서만 attention 계산.
    - 이로써 **윈도우 간 문맥 정보 공유**가 가능해져, 의미적 연속성 확보.
3. **훈련 안정화 기법 (Zero Initialization)**
    - 문제: 초기 학습 단계에서 과도한 변형이 발생하면 학습 불안정.
    - 해결책: **MLP 초기화 방식을 변경**.
        - 일반적으로는 두 개의 선형 계층 A,BA, BA,B의 가중치를 랜덤 초기화.
        - 여기서는 **A는 Gaussian 랜덤 초기화**, **B는 0 초기화**.
    - 수식:x=BAx^
        
        x=BAx^x = \mathbf{B} A \hat{x}
        
        - B = 0에서 시작 → 초기에는 출력이 안정적.
        - 학습이 진행되면서 점진적으로 특징 변환 학습.
    - 결과적으로 **학습 초기의 안정성**을 보장하고 smoother한 학습 진행 가능.

3.2 Image Resampler
목적: **이미지 특징의 중복(redundancy)을 줄이고**, 일정한 길이로 압축하기 위해 사용

- **입력**:
    - Swin/CLIP 같은 비전 인코더에서 나온 이미지 피처 시퀀스 (H×WH \times WH×W 크기 토큰).
    - 보통 토큰 수가 많아 → redundancy 존재.
- **Query (Q)**:
    - 미리 학습 가능한 파라미터 집합으로 정의된 **고정된 개수의 query 벡터**.
    - 예: 길이 256개의 learnable query.
- **Key (K), Value (V)**:
    - 비전 인코더에서 나온 이미지 피처.
- **Cross-Attention 수행**:
    - Query ↔ Key, Value를 연결해서 attention.
    - Query들이 "이미지 전체 피처에서 중요한 정보만 요약"하는 역할.
    - 결과: Query 개수(256개)만큼의 압축된 피처 시퀀스 생성.
- **Positional Encoding 추가**:
    - 단순히 요약하면 위치 정보가 손실되므로,
    - **2D 절대 positional encoding**을 Q-K 페어에 넣어 fine-grained 위치 정보 유지.

- 해상도가 커지면 → 슬라이딩 윈도우로 토큰 수가 **폭발적으로 증가**.
- 언어 모델 입력 길이에 제약이 있고, 학습 시간도 오래 걸림 → 토큰 수 줄이는 게 필요.
- 이미지 해상도를 키우면 세밀한 정보는 늘지만, **중복 토큰(redundant tokens)**도 많이 생김.
- **중복 탐지**:
    - Image Resampler 출력 후 토큰들 중에서 20개를 샘플링 → pairwise cosine similarity 측정.
    - threshold (0.8) 이상인 경우 중복 토큰으로 판단.
    - 중복이 적은 토큰 = 더 “고유(unique)”한 정보 보유.
- **토큰 필터링**:
    - similarity를 기준으로 **중요 토큰(고유 토큰)**만 선택.
    - 단순히 버리는 대신 → 이 중요 토큰들을 **query로 사용**.
- **Cross-Attention 기반 집약**:
    - 중요 토큰(query)이 나머지 모든 토큰(key, value)을 다시 집약.
    - → 정보 손실 최소화하면서 redundancy 제거.

3.4 **Position-Related Task**

문제 

- LLM은 종종 이미지와 상관없는 **환각(hallucination)** 답변을 생성.
- 특히 문서 이미지 기반 QA에서는 답이 이미지 내부 텍스트에 있으므로, 모델이 **위치 정보**까지 인식해야 함.
- **데이터셋 수정**
    - 기존 QA 데이터셋에 **정답이 이미지 안에서 어디에 위치하는지** 좌표 정보 추가.
    - 단, 대화 능력을 유지하기 위해 원래의 QA도 그대로 보존.
    - 즉, QA + QA+좌표 두 버전을 병행.
- **위치 인식 강화 학습**
    - 모델이 텍스트 위치를 이해하도록 **부가적인 학습 태스크** 추가:
        - **Text spotting** (텍스트 위치 검출)
        - **Text reading** (텍스트 인식)
    - 프롬프트 예시는 Tab. 1에 제시 (예: “질문에 답하면서 근거 좌표도 제시하라”).
- **텍스트-좌표 alignment 유지**
    - 항상 “텍스트 → 좌표” 순서로 답변하도록 데이터 정렬.
    - 예: `Answer: Title of chart [373, 930, 531, 967]`
- **좌표 정규화**
    - 다양한 해상도의 이미지를 표준화하기 위해 좌표를 (0, 1000) 스케일로 변환.
    - 변환 공식
    
    ![image.png](attachment:e85de203-efd2-4525-99ee-e684cee8c8c1:image.png)
    
    - 추론 시 inverse operation으로 원래 해상도 좌표 복원.
- 모델이 단순히 "뭔가 plausible한 답"이 아니라, 구체적 근거(좌표)와 함께 답함.
- 문서 기반 QA에서 “위치-텍스트 결합” 능력 강화.

3.5 Dataset Construction

- **Open-source 데이터만 사용**
    - 폐쇄형 사유 데이터 대신 공개 데이터셋만 활용 → reproducibility 강조.
- **Task-specific augmentation**
    - 각 데이터셋 특성에 맞는 증강 기법 적용 (예: OCR 텍스트 증강, 문서 구조 증강 등).
- **Multi-turn QA 변환**
    - 원래 단일 이미지 + 단일 질문 → **multi-turn 대화 형식**으로 변환.
    - 예: 한 이미지에 대해 여러 질문을 연속적으로 던지도록 → 학습속도 빠르게하고 이미지 feature 활용 효율 극대화.
    - 이는 LLaVA [5]가 성공적으로 보여준 방식.

3.6 Loss
Since TextMonkey is trained to predict the next tokens like other LLMs, it only requires maximizing the likelihood of loss at training time.

## 5.2 Chain-of-Thought (사고 과정)

- 여러 데이터셋에서, 모델이 답변 위치까지 같이 제공하도록 요구했을 때 성능 변화를 실험.
- **효과가 있는 경우**:
    - **DocVQA, SROIE** → 답이 이미지 내 텍스트에 직접 있는 경우.
    - 위치까지 요구하면 성능이 눈에 띄게 향상.
- **효과가 없는 경우**:
    - **ChartQA, InfoVQA** → 수치 비교나 정량적 분석이 필요한 경우.
    - 위치까지 요구하면 오히려 성능 저하.
    - 이유: grounding 요구가 추론 과정(reasoning)을 방해했을 가능성.
- **시사점**:
    - 위치 정보 요구 여부는 **질문 유형·데이터셋 성격**에 따라 달라야 함.
- **미래 방향**:
    - 자동으로 **사고 과정(chain-of-thought)**을 생성하는 메커니즘이 유망.
    - 이를 통해 추론 일관성을 높이고 성능도 개선 가능.

- **문서 질의응답** 및 **세밀한 텍스트 분석** 같은 텍스트 중심 태스크 해결.
- 핵심 방법론:
    1. **Shifted Window Attention + Zero Initialization** → 높은 해상도 입력 시 윈도우 간 관계 유지 + 초기 안정성 확보.
    2. **Token Resampler** → 해상도↑ → 토큰 수 폭증 문제를 redundancy 분석으로 해결, 불필요한 토큰 제거.
    3. **멀티 태스크 학습** (텍스트 감지, 인식, 위치 예측 등) → 공간적 관계 이해 향상 → 해석 가능성(interpretability) 강화.
    4. **App Agent 응용** → 스크린샷 내 클릭/위치 인식 지원.
- **실험 결과**:
    - 여러 벤치마크에서 기존 LMM 대비 우수한 성능.
    - 단, 작은 이미지는 해상도를 단순히 올려도 성능이 개선되지 않음.
    - → 따라서 문서에서 크기 변동이 클 때 효율적인 해상도 확장 전략 필요.
