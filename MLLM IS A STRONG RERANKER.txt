MLLM IS A STRONG RERANKER: ADVANCING MULTIMODAL RETRIEVAL-AUGMENTED GENERATION VIA
KNOWLEDGE-ENHANCED RERANKING AND NOISEINJECTED TRAINING

CLIP의 표현 공간은 텍스트-이미지 의미 정렬이 잘 되어 있으며,
다른 모델의 임베딩을 선형 변환으로 CLIP space에 맞추기만 해도,
텍스트 기반 zero-shot 분류가 가능

MLLM한계는 아웃데이트 데이터로 학습된다는 점이다.
이 정적인 특성은 정확하고 최신 대답 능력을 막는다.
Multimodal RAG이 가능성을 제시하지만, 
 the multi-granularity noisy correspondence (MNC) problem 문제 (정확한 검색과 생성을 막음)


MNC 문제의 두 가지 유형
(I) Coarse-Grained Noise (Query–Caption Misalignment)
🔹 문제:
질의(query)에 대한 캡션은 겉보기엔 유사하지만 실제로는 다른 이미지를 설명하고 있음.
→ 즉, "텍스트 간 유사도는 높지만 시각적으로는 다른 이미지"가 검색됨.

🔹 예시:

Query: “Uxmal, Yucatán, Governor's Palace, seen from House of the Old Woman”

Retrieved Caption: “Palacio del Gobernador-Uxmal-Yucatan-Mexico0277 Palace of the Governor in Uxmal”

🔹 결과:
→ 실제 다른 이미지를 가져오지만, retriever는 "텍스트 유사도"만 보고 잘못된 이미지를 선택

🔹 핵심 원인:
텍스트-텍스트 유사도에만 의존하는 retriever의 coarse alignment

(II) Fine-Grained Noise (Query–Image Visual Detail Confusion)
🔹 문제:
쿼리와 매칭된 이미지는 전반적으로 관련이 있어 보이지만,
쿼리에서 요구하는 **세부 시각 요소(fine-grained visual elements)**를 충족하지 않음.

🔹 예시:

Query: “Where is the shadow falling in the image?”

Image: 같은 장소는 맞지만, 그림자가 다르거나 시점이 약간 다른 사진

🔹 결과:
→ generator는 세부사항을 이해하지 못한 채, 부정확하거나 틀린 응답 생성

🔹 핵심 원인:
이미지 내부의 local region-level attention이 부족하거나, 이미지 표현이 너무 global해서 세부 구조를 구분 못함



CLIP은 롱테일 분포(long-tail distribution) 또는 **도메인 특화 용어(domain-specific terms)**에 직면했을 때,
텍스트와 이미지 간의 올바른 쌍을 제대로 매칭하지 못하는 문제가 발생

Long-tail distribution
흔하지 않은 이미지-텍스트 쌍 (예: 희귀 동물, 특이한 지역 건축물 등)

데이터에서 자주 등장하지 않아 CLIP이 학습을 충분히 못했을 가능성 ↑

결과적으로 CLIP은 일반적 개념엔 강하지만, 비주류/희귀 표현엔 약함

🔹 Domain-specific terms
특정 분야(예: 의료, 천문학, 건축 등)의 전문 용어

CLIP의 텍스트 인코더가 일반 도메인에서 학습되었기 때문에
→ 도메인 특화 단어를 시각적으로 어떤 이미지와 연결해야 하는지 잘 모름
이를 시맨틱 이해력이 더 높은 MLLM 도움으로 해결하자
