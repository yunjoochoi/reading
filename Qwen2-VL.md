# **Qwen2-VL**

Qwen-VL 모델의 업그레이드 버전인 **Qwen2-VL 시리즈**에 대해 설명합니다.

**특징:**

1. **Naive Dynamic Resolution (네이티브 동적 해상도):** 기존의 고정된 해상도 방식에서 벗어나, 다양한 해상도의 이미지를 동적으로 처리하여 각기 다른 수의 시각적 토큰으로 변환합니다. 이는 더 효율적이고 정확한 시각적 표현을 가능하게 합니다.
2. **M-RoPE (Multimodal Rotary Position Embedding):** 텍스트, 이미지, 비디오 전반에 걸쳐 위치 정보를 효과적으로 융합할 수 있게 해줍니다.
3. **통합 처리 패러다임:** 이미지와 비디오를 동일한 방식으로 처리하여 모델의 시각적 인식 능력을 향상시킵니다.

**모델 스케일링 및 성능:**

- **스케일링 법칙 연구:** 대규모 비전-언어 모델(LVLM)의 잠재력을 탐구하기 위해 모델 크기(20억, 80억, 720억 파라미터 버전)와 학습 데이터 양을 모두 확장했습니다.
- **경쟁력 있는 성능:** Qwen2-VL 시리즈는 높은 경쟁력을 보이며, 특히 **Qwen2-VL-72B 모델**은 다양한 멀티모달 벤치마크에서 **GPT-4o 및 Claude3.5-Sonnet**과 비슷한 수준의 결과를 달성하며 다른 범용 모델들을 능가

 핵심 기능 요약

- **최고 수준의 시각적 이해 능력:** DocVQA, InfoVQA, MathVista 등 다양한 벤치마크에서 여러 해상도와 화면 비율에 걸쳐 최고 수준의 성능을 달성합니다.
- **장편 비디오(20분 이상) 이해:** 20분이 넘는 긴 비디오를 이해할 수 있으며, 이를 통해 고품질의 비디오 기반 질의응답, 대화, 콘텐츠 생성 등이 가능합니다.
- **견고한 에이전트 기능:** 고급 추론 및 의사 결정 능력을 바탕으로 휴대폰, 로봇 등과 통합되어 시각적 입력과 "텍스트 지시를 기반으로 자율적인 작동을 수행할 수 있습니다.
- **다국어 지원:** 영어와 중국어를 넘어 대부분의 유럽 언어, 일본어, 한국어, 아랍어, 베트남어 등 이미지 내의 다국어 컨텍스트 이해를 지원하여 전 세계 사용자를 대상으로 합니다.

<아키텍쳐>

**1. 나이브 동적 해상도 (Naive Dynamic Resolution)**
Qwen2-VL은 **어떤 해상도의 이미지든 처리**하여 가변적인 수의 시각적 토큰으로 동적 변환할 수 있다. 이를 위해 ViT에서 기존의 절대 위치 임베딩을 제거하고 **2D-RoPE**를 도입하여 2차원 위치 정보를 캡처. 또한, ViT 이후에 간단한 MLP 레이어를 사용해 인접한 2x2 토큰을 단일 토큰으로 압축하여 LLM에 입력되기 전의 토큰 수를 감소. (224x224 이미지는 66개 토큰으로 압축됨)

**2. 멀티모달 로터리 포지션 임베딩 (M-RoPE)**
기존 1D-RoPE를 혁신한 M-RoPE는 위치 정보를 시간(temporal), 높이(height), 너비(width)의 세 가지 구성 요소로 분해하여 멀티모달 입력의 위치를 효과적으로 모델링

- **텍스트:** 세 요소가 동일한 ID를 가져 1D-RoPE와 동일하게 작동합니다.
- **이미지:** 시간 ID는 고정되고, 높이/너비 ID는 토큰 위치에 따라 할당됩니다.
- **비디오:** 프레임마다 시간 ID가 증가하며, 높이/너비는 이미지와 동일한 패턴.을 따릅니다.
이는 위치 정보 모델링을 향상시키고, 위치 ID 값을 줄여 모델이 더 긴 시퀀스에서도 추론을 잘하게함

**3. 이미지 및 비디오 통합 이해**
Qwen2-VL은 이미지와 비디오 데이터를 혼합하여 훈련함. 비디오는 초당 2프레임(2fps)으로 샘플링되며, 3D 컨볼루션(깊이 2)을 통합하여 2D 패치가 아닌 3D 튜브로 처리. 이는 시퀀스 길이를 늘리지 않고도 더 많은 비디오 프레임을 처리할 수 있게 합니다. (이미지는 일관성을 위해 동일한 2개 프레임으로 취급됩니다.) 긴 비디오 처리와 훈련 효율의 균형을 맞추기 위해, 프레임 해상도를 동적으로 조정하여 비디오당 총 토큰 수를 16,384개로 제한합니다.

<훈련 방법- 3단계>

Qwen2-VL은 Qwen-VL을 따라 **3단계 훈련 방법론**을 채택

- **1단계:** ViT (비전 트랜스포머) 컴포넌트만 훈련합니다. 방대한 이미지-텍스트 쌍을 사용하여 LLM 내의 시맨틱 이해를 향상시킵니다.
- **2단계:** 모든 파라미터를 동결 해제하고 더 광범위한 데이터로 훈련하여 포괄적인 학습을 진행합니다.
- **3단계:** ViT 파라미터를 고정(lock)하고, 지시(instruction) 데이터셋을 사용하여 LLM만 독점적으로 파인튜닝합니다.

---

## 사전 훈련 (Pre-training)

- **데이터 구성:** 이미지-텍스트 쌍, OCR, 시각적 질의응답(VQA), 비디오 대화 등 다양한 데이터로 사전 훈련됩니다. 데이터 소스는 정리된 웹 페이지, 오픈소스, 합성 데이터를 포함하며, 지식 컷오프는 **2023년 6월**입니다.
- **초기화:** LLM 컴포넌트는 **Qwen2**의 파라미터로, 비전 인코더(ViT)는 **DFN**에서 파생된 ViT로 초기화됩니다. 단, DFN ViT의 고정 위치 임베딩은 **RoPE-2D**로 대체되었습니다.
- **1S사전 훈련 (초기):** 약 6,000억 토큰을 사용하며, 주로 이미지-텍스트 관계, OCR, 이미지 분류 학습에 중점을 둡니다.
- **2차 사전 훈련 (진행):** 추가로 8,000억 개의 이미지 관련 토큰을 사용합니다. 이 단계에서는 더 많은 혼합 이미지-텍스트 콘텐츠, VQA 데이터셋, 멀티태스킹 데이터셋이 도입됩니다. (언어 능력 유지를 위해 순수 텍스트 데이터도 계속 사용됩니다.)
- **총계:** 사전 훈련 단계에서 누적 **1.4조 개의 토큰**(텍스트 및 이미지 토큰 포함)을 처리합니다. **핵심은 훈련 과정에서 텍스트 토큰에 대해서만 감독(supervision)을 제공한다는 점**입니다.

## Instruction Fine-Tuning

- **형식:** **ChatML** 형식을 사용하여 명령어 수행(instruction-following) 데이터를 구축합니다.
- **데이터:** 순수 텍스트 기반 대화 데이터뿐만 아니라, 이미지 질의응답, 문서 파싱, 다중 이미지 비교, 비디오 이해, 에이전트 상호작용 등 **다양한 멀티모달 대화 데이터**를 포함합니다.
- **목표:** 복잡한 멀티모달 작업을 처리하고 다양한 양식에 걸쳐 광범위한 지시를 이해하고 실행하는 모델의 능력을 향상시키는 것입니다.

<모델 인프라스트럭쳐>

## 저장소 (Storage)

- **데이터 분리:** 텍스트 데이터와 비전 데이터를 분리하여 저장했습니다.
- **텍스트:** Alibaba Cloud **CPFS**(병렬 파일 저장소)에 저장하고 `mmap`을 통해 효율적으로 접근했습니다.
- **비전 (이미지/비디오):** Alibaba Cloud **OSS**(객체 저장소)에 영구 저장했습니다.
- **병목 현상 및 해결:** (특히 긴) 비디오 데이터 디코딩이 주요 병목이었습니다. 여러 시도 끝에 **'캐싱 디코딩(caching decoding)'** 기법을 선택하여 해결했습니다.
- **체크포인팅:** 모델 및 옵티마이저 상태는 CPFS에 저장되었습니다.

## 병렬화 (Parallelism)

- **핵심 전략:** **3D 병렬화**, 즉 데이터 병렬화(DP), 텐서 병렬화(TP), 파이프라인 병렬화(PP)를 조합하여 사용했습니다.
- **메모리 최적화:**
    - DeepSpeed의 **ZeRO-1**을 사용해 옵티마이저 상태를 분할했습니다. (ZeRO-1 (Zero Redundancy Optimizer, 1단계)은 대규모 딥러닝 모델을 훈련할 때 GPU 메모리를 절약하기 위해 '옵티마이저 상태(Optimizer States)'를 여러 GPU에 분할(shard)하여 저장하는 기술)
    - *시퀀스 병렬화(SP)**와 선택적 체크포인팅을 활용해 메모리 사용량을 줄였습니다.
- **텐서 병렬화(TP) 세부 사항:**
    - ViT와 LLM은 함께 샤딩(분할)했지만, 파라미터가 적은 '비전 머저'는 샤딩하지 않았습니다.
    - **문제:** 컨볼루션(convolution) 연산자의 비결정적 동작으로 인해 공유 가중치가 달라지는 이슈가 있었습니다.
    - **해결:** '오프라인 리덕션(offline reduction)'을 수행하여 추가적인 통신(all-reduce) 단계를 피했습니다.
- **파이프라인 병렬화(PP) 세부 사항 (72B 모델):**
    - **1F1B PP** 방식을 사용했습니다.
    - ViT, 비전 어댑터, 일부 LLM 디코더 레이어를 '하나의 스테이지'로 묶고 나머지 디코더 레이어를 균등하게 분할했습니다.
    - **문제:** 데이터마다 비전/텍스트 시퀀스 길이가 동적으로 변했습니다.
    - **해결:** 1F1B 프로세스 시작 전에 이 동적 시퀀스 길이를 '브로드캐스팅'하여 해결했습니다.

## 소프트웨어 (Software)

- **프레임워크:** **PyTorch 2.1.2** 버전과 CUDA 11.8을 사용했습니다.
- **주요 최적화 기법:**
    - **Flash-Attention:** ViT와 LLM 양쪽 모두에 적용하여 훈련 효율을 높였습니다.
    - **Fused Operators (융합 연산자):** `LayerNorm`, `RMSNorm`, `Adam`과 같은 연산을 하나로 묶어 속도를 향상시켰습니다.
    - 행렬 곱셈(matrix multiplication) 중 **통신과 계산을 중첩**

**Straggler Detection**:  분산 컴퓨팅 시스템에서 다른 작업(task)들보다 유난히 느리게 실행되어 전체 작업의 발목을 잡는 '낙오자(straggler)' 작업을 찾아내는 기술→ 하나의 큰 작업을 수백, 수천 개의 작은 작업으로 쪼개어 여러 컴퓨터에서 동시에 처리. 그런데 이 중 단 하나의 작업이라도 비정상적으로 느려지면, 다른 모든 작업이 끝나도 이 '스트래글러' 하나 때문에 전체 작업이 완료되지 못하는 병목 현상

**fused operators** : 예) 일반적인 Adam 업데이트는 (1) 1차 모멘텀(momentum) 계산, (2) 2차 모멘텀(RMSProp) 계산, (3) 편향 보정, (4) 최종 가중치 업데이트 등 여러 단계로 나뉩니다.

Fused Adam은 이 모든 단계를 하나의 GPU 커널로 묶어버립니다.

따라서 가중치, 모멘텀 값 등을 VRAM에서 딱 한 번 읽어와서 모든 계산을 칩 내부에서 끝내고, 최종 업데이트된 값들만 VRAM에 다시 써서 속도가 매우 빨라

Trillion:  **1조**, 100만(million)의 1,000배, 10억(billion)의 1,000배에 해당하는 큰 숫자로, 1,000,000,000,000 (10의 12제곱).
