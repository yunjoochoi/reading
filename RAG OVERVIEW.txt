[1] RAG OVERVIEW
# 나이브 rag
# 단점
Retrieval Challenges: 리트리벌의 정밀도와 재현율이 낮음 즉, 중요정보가 없는 관련없는 청크를 가져옴
Generation Difficulties: 검색된 정보를 활용하더라도, 답변이 질문과 맞지 않거나 할루시네이션 발생
Augmentation Hurdles(정보 통합시의 문제): 검색된 정보를 잘 연결하지 못하면, 앞뒤가 안 맞거나 어색한 답변을 한다. 또 비슷한 내용이 여러 소스에서 중복 검색되면, 답변이 반복적
정보만 반복하고 통찰이 없다.


# Advanced RAG
리트리벌의 퀄리티를 올리고(전, 후 리트리벌), 인덱싱 문제 해결 위해 슬라이딩 윈도우나, 세밀화된 segmentation, 메타데이터 통합 등의 관점으로 접근.
또 검색 프로세스 간소화 위한 옵티마이즈 방법들 도입. 다만 전반적인 프로세스는 나이브와 비슷함.
Pre-retrieval process: 인덱싱 구조/ 원 쿼이 최적화에 초점
-인덱싱 최적화:  데이터 세분성 향상, 인덱스 구조 최적화, 메타데이터 추가, 정렬 최적화, 혼합 검색
-쿼리 최적화: 사용자의 원래 질문을 더 명확하고 검색 작업에 적합하게 만듬
쿼리 재작성, 쿼리 변환, 쿼리 확장
Post-Retrieval Process: 관련 컨텍스트 검색 후에는 쿼리와 효과적으로 통합시키는게 중요.
-청크 리랭킹:정보의 순위를 재지정하여 가장 관련성 높은 콘텐츠를 프롬프트 가장자리에 위치시킨다( 미들: 가장 정보손실 일어나기 때문)
-콘텍스트 압축:모든 관련 문서를 LLM에 직접 입력하면 정보 과부하가 발생하여 핵심 세부 정보에 대한 집중도 낮아기지 떄문에 필수 정보 선택, 중요 섹션 강조, 처리해야 할 맥락 단축에 집중한다.


# Modular RAG
Modular RAG는 기존 RAG보다 정밀한 검색과 문맥 생성을 위해 다양한 모듈을 도입.
1. new modules
- Search Module: LLM이 SQL, SPARQL 같은 쿼리 언어를 자동 생성하여 검색엔진, 데이터베이스, 지식그래프 등 다양한 소스로부터 직접 검색
- RAGFusion: 하나의 질문을 여러 방식으로 재구성하여 병렬 검색하고, 의미 있는 정보를 재정렬하여 더 풍부한 지식을 추출
- Memory Module: LLM이 자체적으로 기억한 정보를 활용해 검색을 더 잘하게 유도, 검색을 점진적으로 개선
- Routing Module: 질문의 종류에 따라 적절한 경로 선택 (예: 요약이 필요한지, DB 검색이 필요한지 등)
- Predict Module: 검색하지 않고 LLM이 직접 문맥을 생성해 중복, 잡음, 무관한 정보를 줄임.
- Task Adapter Module: 각기 다른 태스크에 맞춰 RAG 시스템을 자동으로 조정.
2. New Patterns
- Rewrite-Retrieve-Read: 질문을 LLM이 다시 쓰고, 그걸 기반으로 검색→응답을 생성. → 더 정확한 검색 유도.
- Generate-Read: 검색 없이 LLM이 자체적으로 문맥을 생성한 뒤 그걸 읽고 응답 생성.
- Recite-Read: 모델 내부에 학습된 파라미터에서 정보를 꺼내 읽음. → 사전학습된 지식을 활용.
-Hybrid Retrieval: 키워드, 의미 기반, 벡터 검색을 함께 사용해 다양한 질문에 대응.
-HyDE (Hypothetical Document Embeddings): LLM이 먼저 가상의 답변을 생성하고, 이 답변과 유사한 실제 문서를 찾음. → 더 정밀한 문서 매칭 가능.
- ITERRETGEN / DSP 등 순차 구조 변화: 예: Retrieve → Read → 다시 Retrieve → Read 식으로 반복적으로 검색과 읽기를 수행해 정확도 향상.
- FLARE / Self-RAG: 상황에 따라 검색이 필요 없는 경우는 생략함으로써 효율성 향상.
- Fine-tuning & Reinforcement Learning과 연계: 검색기/생성기를 미세조정하거나 RL로 최적화 가능.

# 프롬프트엔지니어링/ 파인튜닝과 비교됨




[2] RETRIEVAL
A. 검색 소스
: 어떤 **자료(source)**에서, 어떤 **단위(granularity)**로 정보를 검색하느냐에 따라 최종 응답 품질이 달라진다.
초기에는 "텍스트 기반" 소스가 주류- 그 후, 점차 **반구조화된 데이터(PDF)**와 **구조화된 데이터(KG, Knowledge Graph)**로 확대- 최근에는 외부 정보뿐만 아니라 LLM이 스스로 생성한 콘텐츠도 다시 활용하려는 시도가 있다.
PDF와 같이 텍스트와 표 정보가 결합된 데이터를 의미합니다. 반정형 데이터 처리는 두 가지 주요 이유로 기존 RAG 시스템에서 어려움을 야기합니다. 첫째, 텍스트 분할 프로세스가 실수로 표를 분리하여 검색 중 데이터 손상을 초래할 수 있습니다. 둘째, 데이터에 테이블을 통합하면 의미적 유사성 검색이 복잡해질 수 있습니다.
 -반구조화된 데이터를 처리할 때 한 가지 방법은 LLM의 코드 기능을 활용하여 TableGPT[85]와 같은 데이터베이스 내 테이블에 대해 Text-2-SQL 쿼리를 실행. 
 -텍스트 기반 방법을 사용하여 테이블을 텍스트 형식으로 변환하여 추가 분석을 수행
::  그러나 이 두 가지 방법 모두 최적의 솔루션은 아니다.'

Unstructured Data (비구조화 데이터): 일반적인 텍스트 기반 데이터
Semi-structured Data (반구조화 데이터): 텍스트 + 표가 섞여 있는 문서, 대표적으로 PDF
Structured Data (구조화 데이터): 정보가 검증되고 정확도가 높음
예시: KnowledGPT → LLM이 KG 쿼리 자동 생성, PCST 최적화 문제로 그래프 내 핵심 정보만 검색, LLM을 “soft prompting” 방식으로 그래프 질문에 최적화
2) Retrieval Granularity 검색 단위
문장, 문단, 문서 등

B. Indexing Optimization 인덱싱 최적화
문서가 분할되고 임베딩되어 벡터디비에 저장되는 과정이다.
 1.청킹 전략
- 고정 토큰수
-작은 청크는 필요한 컨텍스트를 완전히 전달 못할 수도 있지만 노이즈는 적음
 2. 메타데이터 첨부
 -metadata can also be artificially constructed: 문서 요약, 
 3. 구조 색인
- 계층적 색인 구조: 문서 계층 구조 구축 시 KG사용하면 일관성 유지에 도움이 된다.
- 지식 그래프를 문서 인덱스로 활용하면 정보 간 연결성을 명확히 하고, 검색 과정의 정확도와 생성 응답의 일관성을 높인다.
- RAG에서 단일 문서가 아닌 여러 문서 간 추론이 필요한 경우, 정보를 효과적으로 연결하고 검색


C. Query Optimization
나이브 rag의 사용자의 원래 질문 의존성이 높다는 특징은 여러 단점이 있는데, 대표적으로 질문의 불명확성, 복잡성, 모호성(동음이의어 등) 등이 있다.
1) 질문 확장: 원래 질문을 여러 개의 풍부한 관련 질문으로 확장(예: 단순 질문 “삼성전자 실적은?” → “삼성전자 2024년 1분기 매출은?”, “EPS는?”, “전년 대비 변화는?”처럼 LLM을 활용해 프롬프트 엔지니어링으로 확장)
- Sub-Query Planning: 복잡한 질문을 더 단순한 질문들로 나눔(예: AI 산업의 미국 내 정책 변화가 기업 수익에 미치는 영향?① AI 산업 관련 정책은?, ② 해당 정책 변화 내용은?, ③ 기업 수익에 어떤 영향? 식으로 간단한 질문부터 차근차근 해결해 전체 질문 해결)
- Chain-of-Verification: 확장된 질문들이 단순히 생성되고 끝나는 것이 아니라, LLM이 다시 한 번 검토(검증)하여 신뢰도 높은 쿼리만 남김
서브쿼리들의 검증 결과를 받고, 관련 있는 서브쿼리만 재실행. => 비용과 시간지연 크다

2) Query Transformation
- Query Rewrite (질문 재작성): 현실의 질문들은 구조가 안 좋거나 불명확한 경우가 많음. 그래서 LLM 또는 소형 모델(RRR 등)을 사용해 질문을 더 명확하고 검색 친화적인 형태로 재작성
- HyDE (Hypothetical Document Embedding): 질의 자체를 벡터화하는 대신, LLM이 "가상의 정답" 문서를 먼저 생성하고 그 문서와 유사한 실제 문서를 검색하는 방식.
- Step-back Prompting: 원래 질문을 한 단계 추상화해서 더 일반적인 상위 질문을 생성 (예: "이 질문이 의미하는 핵심은?") 그런 다음: 원 질문 + step-back 질문 → 둘 다 검색에 사용

3) Query Routing (질의 라우팅)
- Metadata Router / Filter: 질문에서 엔터티(키워드)**를 추출한 뒤, 청크의 메타데이터 (문서 출처, 주제 등)를 기반으로 필터링
- Semantic Router: 질의의 의미를 분석해서 가장 적합한 라우트(RAG 구성)로 전달



D. Embedding
- 희소 인코더: 매우 큰 차원의 0이 많은 희소 벡터, 단어의 출현 빈도 기반, 단어 단위 기반이라 문맥 반영 불가, 빠르고 학습 필요 없다. ex. BM25, TF-IDF (키워드 기반, 사용자가 입력한 키워드와 가장 관련성이 높은 문서를 찾는 데 도움)
- 밀집 인코더: 의미 기반 실수 벡터, 사전학습된 언어 모델의 의미 인코딩, 의미 유사성 기반, 훈련비용 수반 ex. BERT
1. 믹스/하이브리드 리트리벌
희소, 밀집 임베딩은 각각 다른 특징을 캡쳐 가능하기에 상호보완하기 위해 둘 다 사용
희소 임베딩읜 밀집 임베딩의 제로샷 검색을 향상시킨다.
2. 파인튜닝 임베딩 모델
컨텍스트가 프리트레이닝 코퍼스와 매우 다를 때, 특히 의료나 법 등 전문용어가 만연한 분야에서 불일치를 완화하기 위해 자체 도메인 데이터세트에 대한 임베딩 모델을 미세 조정한다. 
3. LLM결과를 미세 조정을 위한 감독용으로 사용 (LSR)
LM 기반 피드백을 활용하여 강화 학습을 통해 검색기를 강화.
# DPR: dense passage retrieval, 문서와 사용자 질의를 각각 임베딩하고 유사도 높은(서로 관련이 있는) 문서를 리턴.
지도 학습 방식으로 훈련되고, QA에 강력함.

E. Adapter
파인튜닝의 비용적 어려움을 해소하기 위해 외부 어댑터를 통합하는 방법


[3] GENERATION
리트리벌 후 최적화 과정- 긴 컨텍스트는 최악. Lost in the middle problem 최소화.
A. Context Curation
1) Reranking: 중요 리트리벌 결과가 돋보이게
rule-based methods: depend on predefined metrics like Diversity, Relevance, and MRR
model-based: like Encoder-Decoder models from the BERT series
specialized reranking models: Cohere rerank or bge-raranker-large, and general large language models like GPT
2) Context Selection/Compression: 너무 긴 맥락은 노이즈 유발.
소규모 언어 모델(SLM)을 활용하여 중요하지 않은 토큰을 감지하고 제거하여 인간이 이해하기 어렵지만 LLM은 잘 이해할 수 있는 형태로 변환

B. LLM Fine-tuning
LLM이 특정 데이터 형식에 적응하고 지시된 대로 특정 스타일로 응답을 생성하게 학습
혹은 큰 모델에서 지식 증류
