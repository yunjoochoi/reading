1. 멀티모달 VLM의 작동원리 연구

2. LLM ->SLM distillation

3. 만화 인페인팅



design of VLMs are often not justified-설계와 관련된 중요한 결정들이 정당한 근거 없이 이루어지는 경우가 많음
근거 없는 결정들이 어떤 선택이 실제로 모델 성능을 향상시키는지 식별하기 어렵

비전-언어 모델을 구축할 때 진정으로 중요한 것은 무엇인가?
연구에서 다른 선택이 이루어지는 두 가지 주요 영역
(a) 모델 아키텍처- 특히 커넥터 모듈 (b) 멀티모달 학습 절차

발견
1. 비전-언어 모델의 진전은 단일 모달 사전학습 백본(unimodal backbone)의 발전에 크게 의존
2. 완전 자기회귀(fully autoregressive) 아키텍처가 교차-어텐션 아키텍처보다 우수한 성능을 내지만, 안정적인 학습을 위해 최적화 과정의 수정이 필요
3. 사전학습된 비전 백본과 텍스트-비전 모달리티를 연결하는 모듈의 adaptation은 한편으로 추론 시 효율성을 높이고, 다른 한편으로는 이미지를 원래의 비율과 크기로 처리하면서도 다운스트림 성능을 저해하지 않는다.
4. 이미지 처리 방식의 수정은 추론 비용과 다운스트림 성능 사이의 트레이드오프를 가능하게 한다.

사전학습 백본의 선택적 언프리징, 데이터 반복, 그리고 학습 데이터 조합이 제로샷 및 퓨샷 성능에 미치는 영향 등에 대한 통찰


3 Exploring the design space of vision-language models
비전 백본과 언어 백본의 선택이 최종 VLM 성능에 어떤 영향을 미칠까요?
언어 모델을 더 나은 것으로 바꾸는 것이 VLM 성능 향상에 가장 큰 효과를 보였습니다.


완전 자기회귀 아키텍처와 교차-어텐션 아키텍처 비교
1. Frozen 백본 조건 (백본 고정 학습)

교차-어텐션은 더 자주 cross-attention 블록을 삽입할수록 성능이 향상됨 (Alayrac et al., 2022).

이 경우, 교차-어텐션 아키텍처는 완전 자기회귀보다 13억 개(trainable) 파라미터가 많음 (총 20억).

추론 시 FLOPs도 10% 더 많음.

따라서 이 조건에서는 교차-어텐션이 7점 더 성능 우수.

다만, trainable 비율은 낮음:

Fully autoreg. → 전체 파라미터 중 15%만 학습

Cross-attention → 25% 학습

→ 이 낮은 비율이 표현력 부족 및 성능 제한 요인이 됨.

2.백본 전체 Unfreeze 조건

모든 파라미터(새로 초기화된 + 사전학습 백본)까지 풀어 학습 시:

완전 자기회귀는 학습 불안정(손실 발산) 문제 발생 → learning rate 조정, 단계적 unfreeze로도 안정화 실패.

해결책 → LoRA (Low-Rank Adaptation, Hu et al. 2022) 적용:

새로 초기화된 파라미터는 full fine-tuning, 사전학습 백본은 LoRA 적용.

결과적으로 안정적 학습 가능.

성능 변화:

Fully autoreg. → +12.9점 향상

Cross-attention → +0.6점 향상

따라서, Frozen일 때는 교차-어텐션이 낫지만, LoRA 기반 전체 unfreeze 조건에서는 완전 자기회귀가 더 우수.

Frozen 상태 → 교차-어텐션이 유리

Unfreeze 상태 → 완전 자기회귀가 더 강력

단, Unfreeze 시 불안정성 문제 → LoRA로 해결
