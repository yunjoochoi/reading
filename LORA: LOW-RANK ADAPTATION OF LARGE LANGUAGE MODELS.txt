LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

<ABSTRACT>
NLP 파라다임은 일반적인 도메인에 대해서 큰 스케일로 프리트레이닝
우리가 더 큰 모델을 프리트레인하고 풀 파인튜닝할수록 실현가능성이 낮아진다.
그래서 사전 학습된 모델 가중치는 고정하고, 학습 가능한 순위 분해 행렬을 트랜스포머 아키텍처 각 계층에 주입하는 저순위 적응(Low-Rank Adaptation, LoRA) 제안.
다운스트림 작업을 학습가능한 매개변수 수를 크게 줄이면서 최적화함.



1 INTRODUCTION
자연어 처리 분야의 많은 애플리케이션은 하나의 대규모 사전 훈련된 언어 모델을 여러 다운스트림 애플리케이션에 적용하는 데 의존.
미세 조정의 주요 단점은 새 모델이 원래 모델과 같은 수의 매개변수를 포함한다는 점. (비효율적)
이제까지의 방식은 1. 일부 매개변수 조정 또는 2. 새로운 태스크에 대해 외부 모듈 학습 방식으로 비효율 해결하려 했다.
=>배포시 효율성 향상되지만, 모델 깊이를 늘리거나, 모델의 사용하능한 시퀀스 길이 줄임으로써 추론 지연 발생

over-parameterized models(거대모델)의 경우 이론상으로 많은 파라미터를 모두 조정할 수 있지만, 실제로는 그렇게 안된다.
결국엔 학습 결과가 아주 저차원의 부분(파라미터 서브셋)에 모여 있다.
=> 그럼 차라리 저차원 공간만 학습하자는게 Lora가 나오게 된 계기

장점들
사전 훈련된 모델 하나로 여러 다운스트림 태스크에 맞는 로라 모듈 구축 가능-저장공간 줄이고, 작업전환 오버헤드 줄임
대부분의 매개변수에 대해 기울기를 계산하거나 옵티마이저 상태를 유지할 필요가 없기 때문에 적응형 옵티마이저 사용시 학습 효율성 크게 높임
LoRA는 학습할 때만 작은 행렬을 따로 관리하지만 배포시에는 원래 가중치에 합쳐버릴수 있다, 그래서 추론지연 없음
LoRA는 기존의 여러 fine-tuning 방법들과 독립적(orthogonal) 이라서, 다른 방법들과 함께 사용될 수 있다. (Prefix-Tuning 등)


2 PROBLEM STATEMENT
frozen된 원래 모델 위에 저차원 변화만 더하는 방식으로 학습
확률 분포 공간은 그대로인데, 그 공간 안에서 작은 저차원 변형만 적용
LoRA는 확률 분포 공간 자체를 다시 만드는 게 아니라, 기존 공간 안에서 일부 제한된 방향으로만 weight를 살짝 움직여서 output 분포를 조정하는 방법이다.
*기존의 파인튜닝(full fine-tuning)은 새로운 확률 분포 공간 자체를 학습(프리트레인된 모델이 만든 확률 분포 공간을 완전히 다시 수정하며 다운스트림 태스크에 맞는 확률 분포 공간 재생성)
학습가능한 파라미터 크기는 원 모델의 100분의 1 수준

ΔW를 low-rank decomposition하면, Full fine-tuning시 파라미터 수보다 훨씬 줄어듬
O(dk) → O(r(d+k))
A는 d×r, B는 r×k.
r은 여기서 매우 작은 수므로 크게 파라미터 수를 줄인다.



3 AREN’T EXISTING SOLUTIONS GOOD ENOUGH?
효과적인 adaptations을 위한 여러 시도가 있었다.
1) 어댑터 계층 추가나, 2) 인풋레이어의 활성화의 일부 형태를 최적화하는것
지연시간과 규모의 관점으로 봤을 때 그러나 두 전략 모두 한계.
어댑터 계층의 추가 컴퓨팅을 우회하는 직접적인 방법은 없고, 프롬프트를 조정(prefix tuning)해서 모델을 학습시키는 게 쉽지 않다. Prefix Tuning은 입력 시퀀스 앞부분에 학습 가능한 벡터(prefix)를 붙이는 방식인데,
이 방법은 최적화가 어렵고 성능이 일정하지 않음. 게다가 prefix가 입력 공간을 차지해서 원래 태스크에 쓸 수 있는 정보가 줄어들기 때문에 성능 저하의 원인이 될 수 있다.




4 OUR METHOD
4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES
신경망은 행렬 곱셈을 수행하는 여러 개의 밀집 레이어(완전 연결 계층)로 구성되는데 이러한 계층의 가중치 행렬은 일반적으로 풀랭크이다. 즉, 모든 행(또는 열) 벡터가 서로 독립(independent)이다. 모든 차원을 벡터스페이스를 표현하는데 사용중이기 때문에 차원 축소나 압축이 불ㄹ가능.
=매트릭스 내 모든 벡터가 공간에 대한 설명을 가지고 있어서 더 압축하거나 차원 축소 불가능

하지만 weight 자체는 full-rank라도, task-specific 변화량(ΔW)은 아주 소수의 중요한 방향(저차원 r차원) 만 필요

A는 랜덤 가우시안 초기화(평균 0, 분산 1), B는 0으로 초기화. - 학습 초기에 기존 모델의 출력이 그대로 유지되어 프리트레인된 모델의 출력을 변형 않고 부드럽게 시작.

A Generalization of Full Fine-tuning: LoRA의 r을 키워서 학습하는 파라미터 수를 늘려주면, 결국 LoRA 학습은 full fine-tuning과 거의 같아진다.
LoRA는 r을 늘리면 full 모델 학습에 거의 가까워지지만, Adapter나 Prefix 방법은 r을 늘려도 구조적인 한계 때문에 full fine-tuning처럼 될 수 없음.

No Additional Inference Latency: 이미 파라미터 계산 다 해놓은 상태로, 다른 다운스트림 태스크로 전환할때 기존 BA빼고 새 BA 더하면 됨! (파인튜닝도 inference latency없다)



4.2 APPLYING LORA TO TRANSFORMER
신경망 안의 어떤 weight matrix에라도 LoRA를 적용할 수 있지만, 로라는 특정 레이어만 골라 적용
Transformer 모델 안에는 셀프어텍션 블록에 4개 레이어, 피드포워드 부분에 2개 레이어 있지만 로라에서는 어텐션 레이어 4개에만 적용. MLP 모듈은 전부 freeze.
특히 어텐션 블록 레이어 중 3개는 멀티헤드 어텐션 떄문에 실제론 여러 헤드로 나뉘지만 로라에서는 이걸 그냥 하나의 큰 매트릭스로 취급. 구현 단순화 위해 헤드로 안쪼개고 전체 가중치 하나로 로라 적용.

Practical Benefits and Limitations
<장점> 메모리 사용량 줄임
Transformer 모델을 Adam 옵티마이저로 학습할 때, 기존 weight들은 freeze하니까 optimizer states 저장량 줄었다.
또, r=4로 설정하고,Query와 Value projection matrix에만 LoRA를 적용하면, 체크포인트 파일 크기가 10,000배 줄어든다. =>결과를 저장할 때도 엄청나게 경량화.
배포 중에 task를 전환할 때 전체 weight를 다 교체하는 게 아니라, LoRA weight (A, B)만 교체하면 되기 때문에, 전환 비용 낮다.

<단점>여러 LoRA 모듈을 동시에 batch 처리하기 복잡, 특히 merge 이후에는 어려움.
merge: 학습이 끝난 뒤, 원래 모델 weight에 LoRA의 업데이트 ΔW=BA를 미리 더해서 합치는 것
만약 merge를 안 하면, matmul 2번. merge를 하면, 그냥 Wx 한 번만 계산하면 되서 원래 모델처럼 작동.
머지를 하면 inference 속도는 빨라지지만, 머지를 안 하면 task switching이 매우 빠르고 유연해지는 tradeoff가있다.
