Tokenizers

1. BPE
원래는 텍스트 압축 알고리즘이었다. 아이디어는 자주 등장하는 문자쌍을 하나의 토큰으로 치환하고, 반복적으로 적용해서 단어 조각 단위로 표현한다.

텍스트를 먼저 캐릭터 단위로 잘게 나누고 모든 바이그램 등장 빈도를 센다. 이후, 가장 많이 등장한 문자쌍을 하나의 심볼로 합친다. 이를 원하는 vocabulary (단어집합) 크기에 도달할 때까지 반복한다.

장점- 자주 등장하는 조각은 하나로 묶어 압축/ 희귀 단어도 조각으로 표현
단점- 단어 경계 고려 안함. (the mat을 them으로 병합) /Merge 순서가 잘못되면 비자연스러운 토큰(inter → in + ter로 분리)

Byte-Level BPE: BPE방식애서 문자열을 UTF-8 바이트 단위로 변환. 
OOV (Out of Vocabulary) 문제가 완전히 사라진다


 2. WordPiece
BPE에서 발전된 방식으로 BERT에 사용되었따. 가능도를 기준으로 병합을 결정한다. 단순 빈도가 아닌, 모델의 예측 정확도 높이는 쪽으로 병합.

문자를 캐릭터 단위로 토큰화한 후, 두 토큰을 합쳐 새로운 토큰 만들었을때 전체 데이터 문장의 문장 발생 확률이 얼마나 높아지는지 본다. 이를 원하는 vocab 사이즈까지 반복.

장점- 언어적으로 BPE보다 자연스럽다 / 희귀 단어 처리 가능 / 접두사, 접미사 잘 처리
단점- 계산량 많다.



3. SentencePiece
텍스트를 공백 기준 없이 통째로 처리한다. 즉, 공백을 포함
BPE 모드로 학습시 BPE처럼 merge를 반복한다. Unigram LM 모드로 학습 시 가상의 커다란 토큰 집합에서 시작해서 가장 문장 확률이 높은 서브셋을 점진적으로 선택한다.

장점- 사전 전처리 없이 바로 raw 텍스트를 처리 / 언어 독립적 / 공백, 유니코드 정규화 지원
단점- 모델 자체 크기가 커질 수 있다.
