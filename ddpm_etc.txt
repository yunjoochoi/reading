ddpm은 다른 타임스텝의 노이즈 적용된 여러 이미지들로부터 유넷을 통해 노이즈 예측하므로써 
원래 데이터 x0를 순차적으로 생성해낸다.
노이즈 스케줄(베타)는 임의적이며 누적알파곱은 원래 데이터가 얼만큼 유지되었는지 의미한다.
딱히 조건안걸었기 때문에 학습된 이미지 분포로부터 비슷한 이미지 하나를 생성해낸다. 
loss는 DDPM 모델이 학습 시 예측한 노이즈와 실제로 추가했던 노이즈 간의 차이

stable diffusion의 변화

DDPM기반인데
1. latent space에서 학습(생이미지를 먼저 vae로 압축시 한번 사용하고, 디퓨전으로 복원한 latent를 이미지로 복원할때 한번 사용)하여 고해상도 픽셀 처리 가능
2. 유넷중간에 크로스어텐션 넣어서 텍스트 컨디셔닝 가능하게 함
3. classifier free guidance: 조건 있는 샘플링과 조건 없는 샘플링(ddpm처럼 전제학습 이미지 분포에서 하나 뽑게 하는거) 혼합해서 텍스트 조건 더 강하게 반영

그리고 원래 ddpm에서 있었던 self어텐션이 사라진건 아니고, 크로스어텐션도 있고 셀프어텐션도 공존한다.


<Denoising Score Matching >
score: 확률 분포의 로그를 x로 미분한 값으로, 데이터가 더 있을 법한 방향을 알려준다.
확률밀도함수p의 로그함수에 대한 기울기를 score라고 한다.
이 score는 어떤 위치 x에서, 데이터가 있을 법한 방향을 알려주는 벡터

근데 p를 직접 모델링할순없다. 그래서 로그 확률의 기울기인 score만 직접 모델링하자는 것이 score matching
데이터는 실제 데이터는 고차원 공간 상의 일부 지점에서만 존재하고, 전체 공간에서 매우 희박(sparse)하기 때문에, 모든 x에 대한 score를 직접 학습하는 건 어렵다. 그래서 노이즈가 섞인 데이터를 보고 원래 데이터를 복원하는 방향을 학습해서 score를 우회 학습= 노이즈를 예측하는 방식.
복원 방향은 곧 score 함수의 근사값으로 해석 가능

<CFG-Classifier-free guidance>
훈련시 조건부 모델을 학습할 때 일정 비율로 조건 c를 드랍해서 학습한다.
즉 두가지를 모두 학습
예: 텍스트를 안 주는 대신 "" (빈 프롬프트)

샘플링 시, 조건 있는 예측 노이즈와 조건 없는 예측 노이즈를 합쳐서 사용한다
가이던스 스케일 곱해서 이를 조정.
조건 안줬을때의 평균적인 예측을 빼서 조건 줬을때의 예측 더 강조.


<Zero Terminal SNR>
SNR = Signal-to-Noise Ratio, 신호 대 잡음 비율
마지막 디퓨전 스텝 (t = T)에서 SNR이 0이 되도록 설계
Zero Terminal SNR 필요성: 샘플링의 출발점이 진짜 노이즈여야 학습과정과 잘 이어짐


deterministic sampling
Denoising Diffusion Implicit Models: DDPM의 확률적 샘플링을 deterministic(비확률적)하게 바꾼 샘플링 기법
