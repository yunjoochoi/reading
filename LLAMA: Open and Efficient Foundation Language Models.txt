LLAMA: Open and Efficient Foundation Language Models

Weight Decay: 학습할 때 가중치(Weight)가 지나치게 커지는 것을 막기 위해, 가중치에 감쇠항 추가
L2 정규화(Regularization)와 동일

큰 가중치는 모델이 데이터에 과적합(Overfitting) 될 위험이 커짐 → 작은 변동에도 예민해져서 일반화가 잘 안 됨

Activation Function: 가중합을 비선형 함수로 변환하여 신경망이 복잡한 패턴 학습 가능하게 함
optimizer: 손실 함수를 최소화하도록 가중치를 업데이트하는 방법

모델크기 스케일링이 가장 좋은 결과는 아님 모델크기는 작지만 더 많은 데이터에서 좋은 결과 나옴
작은 모델에 데이터 늘릴수록 성능 향상 

라마에서 사용한 학습방법
1. 학습 안정화 위한 input 정규화 (아웃풋 정규화 아님)
2. swiGLU
3. RoPE
