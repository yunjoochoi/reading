Late Chunking: 긴 컨텍스트를 청킹할 때, 
1. 청킹 후 임베딩 방식이 아니라, 임베딩 후 청킹 방식(Sentence Embedding 후 문장 간 유사도 비교를 통해 유사도가 낮은 곳에서 청킹) 혹은,
2. 문서 전체를 LLM에게 주고 의미 단위로 나누게 (최대 컨텍스트 길이가 긴 모델을 사용하는 것이 좋음)

Chunked Pooling:  긴 문서를 일정 길이나 의미 단위로 나누고, 각 청크를 하나의 벡터로 계산한다. 모든 청크에 어플라이 한 후, 이 벡터들을 평균이나 정규화 등을 거쳐 전체 문서의 임베딩으로 사용
청크 내부 의미들이 평균화되거나 희석되며 세밀한 문맥 표현은 사라지지만, 긴 문서 전체를 단일 벡터로 표현 가능.
문서 분류, 검색 등의 태스크에는 매우 효과적 
Attention Pooling: 쿼리와 연관된 청크에 더 집중하는 가중합 방식'


# 코드1
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel("models/gemini-1.5-pro")

def late_chunk_with_llm(document_text, max_chunk_count=5):
    prompt = f"""
다음 문서를 의미 단위로 {max_chunk_count}개 이하의 청크로 나누세요. 각 청크는 자연스러운 단락이나 주제 기준으로 나눠야 하며, 각 청크를 # 청크 N 형식으로 구분해 주세요.

문서:
{document_text}
"""
    response = model.generate_content(prompt)
    chunks = response.text.split("# 청크")[1:]  # "청크 1", "청크 2" ...
    return [chunk.strip() for chunk in chunks]

# 사용 예시
with open("document.txt", "r", encoding="utf-8") as f:
    text = f.read()

chunks = late_chunk_with_llm(text)
for i, chunk in enumerate(chunks, 1):
    print(f"청크 {i}:\n{chunk[:300]}...\n")



# 코드2: Sentence Embedding 기반 Late Chunking
from sentence_transformers import SentenceTransformer, util
import numpy as np

model = SentenceTransformer("intfloat/multilingual-e5-base")

def late_chunk_by_semantics(document_text, threshold=0.7):
    from nltk import sent_tokenize
    sentences = sent_tokenize(document_text)
    embeddings = model.encode(sentences)

    # 유사도 기반 분할
    chunks, current_chunk = [], []
    for i in range(len(sentences)):
        if i > 0:
            sim = util.cos_sim(embeddings[i], embeddings[i - 1])
            if sim < threshold:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
        current_chunk.append(sentences[i])
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

# 사용 예시
chunks = late_chunk_by_semantics(text)
for i, chunk in enumerate(chunks, 1):
    print(f"청크 {i}:\n{chunk[:300]}...\n")


# 토크나이즈: 문장을 작은 단위(토큰)로 나누는 작업
# 청킹(Chunking): 토큰화 후 문법적 의미가 있는 덩어리(Chunk)로 묶는 작업
