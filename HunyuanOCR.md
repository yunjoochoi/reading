## 1. 개요
* 대규모 모델용 강화학습을 경량 OCR 모델 1B에 적용
* 모바일 및 엣지 환경에서 효율적이고 정확한 텍스트 이해 가능성 증명
* 독자적 RLVR 전략으로 Qwen-235B, Gemini-2.5-Pro 등 거대 모델 상회

## 2. 4단계 훈련 파이프라인 (4-Stage Training)
강화학습 이전에 모델의 기초 성능을 확립하는 체계적인 사전 훈련 과정

### Stage 1: 시각-언어 정렬
* **목표**: 시각 인코더와 LLM 간의 기본 연결 구축
* **구성**: ViT 및 어댑터 학습, LLM 동결
* **데이터**: 이미지 캡션 등 50B 토큰, 8k 컨텍스트

### Stage 2: 멀티모달 사전 훈련
* **목표**: 다양한 OCR 작업 동시 수행 능력 배양
* **구성**: LLM 포함 전체 모델 학습 Unfreeze
* **데이터**: 스포팅, 파싱, 번역 등 300B 토큰, 8k 컨텍스트

### Stage 3: 긴 문맥 사전 훈련
* **목표**: 논문, 전공 서적 등 긴 호흡의 문서 처리
* **구성**: 컨텍스트 윈도우 32k 확장
* **데이터**: 긴 문서 파싱 데이터 80B 토큰

### Stage 4: 애플리케이션 중심 SFT
* **목표**: 사용자 지시 준수 및 포맷 엄수
* **구성**: 32k 컨텍스트 유지, 미세 조정
* **데이터**: 고품질 라벨링 데이터 및 오답 유도 데이터 24B 토큰

## 3. 강화학습 (RL) 상세

### 접근 전략: 이원화된 보상 체계
작업 특성에 따라 두 가지 보상 방식을 혼합하여 최적화

1.  **RLVR (Closed-form)**: 스포팅, 파싱 등 정답이 명확한 작업. 검증 가능한 규칙 기반 보상 부여
2.  **LLM-as-a-judge (Open-ended)**: 번역, VQA 등 개방형 작업. 고성능 LLM이 심판이 되어 점수 부여

### 알고리즘: GRPO
* **개념**: Group Relative Policy Optimization
* **방식**: 단일 질문에 대해 답변 그룹 생성 후 그룹 내 상대평가 수행
* **장점**: 별도의 Value Model 없이 효율적인 메모리 사용 및 학습 안정성 확보

### 보상 설계 공식 Reward Design
* **스포팅**: `Mean(IoU 매칭 - 정규화된 편집 거리)` 구조. 위치와 글자 정확도 동시 반영
* **파싱**: `1 - 정규화된 편집 거리`. 구조적 무결성 및 내용 정확도 평가
* **VQA**: 정답 시 1, 오답 시 0 이진 보상. 스타일보다 팩트 일치 여부 중시
* **번역**: LLM 평가 0~5점을 [0, 1]로 정규화. 특히 2~4점 구간 세밀도 확장으로 미세 품질 학습 유도

### 제약 조건 Constraints
* **길이 제한**: 최대 길이 초과 시 즉시 보상 0점
* **포맷 준수**: JSON 등 스키마 위반 시 즉시 보상 0점. 환각 방지 및 형식 강제

## 4. 데이터 큐레이션 및 생성

### 3단계 필터링 파이프라인
고품질, 다양성, 난이도를 고려한 정교한 데이터 선별

1.  **품질 Quality**: 오픈소스 및 합성 데이터 결합. LLM 기반 심사로 이미지-텍스트 정렬 확인 및 악용 가능 문제 제거
2.  **다양성 Diversity**: 출력 다양성이 낮거나 보상 분산이 0인 학습 효과 없는 샘플 제거
3.  **난이도 Difficulty**: 모델 통과율 기반 필터링. 너무 쉽거나 해결 불가능한 극단적 샘플 제외

### Wild 데이터 생성
실제 환경 적응력을 높이기 위한 물리적 데이터 증강 기법

* **방식**: 원본 문서를 인쇄 후 수동으로 구김, 접힘, 조명 변화 가함
* **목적**: 캡처 시 발생하는 현실적 왜곡 시뮬레이션 및 강건성 확보

## 5. 실험 결과

### 문서 파싱 성능
* **OmniDocBench**: 디지털 및 Wild 환경 모두에서 최고 성능 기록
* **DocML**: 한국어 포함 14개 언어 테스트 전 부문 SOTA 달성

### 정보 추출 및 VQA
* **핵심 정보 추출**: 신분증 등 30개 카테고리 1위. Qwen-235B 등 초대형 모델 상회
* **비디오 자막**: 자막 추출 작업 최고 정확도 기록
* **OCRBench**: 동급 DeepseekOCR 압도, Qwen3VL-2B 모델과 대등

## 6. 결론
* 4단계 사전 훈련으로 기초 체력 확보 후 RLVR과 GRPO로 성능 극대화
* 정교한 데이터 큐레이션과 엄격한 보상 설계로 1B 경량 모델 한계 극복
* 모바일, 엣지 환경에서 거대 모델급 OCR 및 파싱 성능 실현
