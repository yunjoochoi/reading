1. 개요
이 논문은 대규모 추론 모델에 주로 사용되던 강화학습을 경량 OCR 모델에 적용하여 모바일 및 엣지 환경에서도 효율적이고 정확한 텍스트 이해가 가능함을 증명합니다. 특히 HunyuanOCR 1B 모델은 독자적인 RLVR 전략을 통해 특정 작업에서 Qwen-235B, Gemini-2.5-Pro 등 거대 모델을 능가하는 성능을 보여줍니다.

2. 핵심 방법론
강화학습 전략
모델 크기는 줄이되 성능을 극대화하기 위해 작업 성격에 따라 두 가지 RL 접근 방식을 혼합하여 사용합니다.

정답이 명확한 작업 Closed-form Tasks

대상: 텍스트 스포팅, 문서 파싱

방법: 검증 가능한 보상 기반 강화학습인 RLVR을 적용하여 구조적 특성을 활용.

개방형 작업 Open-ended Tasks

대상: 번역, 텍스트 기반 VQA

방법: LLM-as-a-judge 접근법을 사용하여 고성능 LLM이 경량 모델의 출력을 평가하고 보상을 제공.

데이터 큐레이션
고품질 학습 데이터 확보를 위해 다음 3단계 파이프라인을 구축했습니다.

품질: 오픈소스 및 합성 데이터를 결합한 뒤 LLM 심사를 통해 이미지와 텍스트 정렬을 확인하고 악용 가능한 단순 문제를 필터링합니다.

다양성: 모델 출력이 너무 단조롭거나 보상 변화가 없어 학습 가치가 낮은 샘플을 제거하여 탐색 공간을 확보합니다.

난이도: 모델의 통과율을 기반으로 너무 쉽거나 해결 불가능한 문제를 제거하여 난이도 균형을 맞춥니다.

보상 설계
작업 유형별로 최적화된 능력 적응형 보상 공식을 사용합니다.

스포팅: 위치 정확도인 IoU와 글자 정확도를 동시에 평가합니다.

파싱: 문서 구조 및 내용의 무결성을 확인하기 위해 정규화된 편집 거리를 사용합니다.

VQA: 의미적으로 정답과 일치하면 1점, 아니면 0점을 부여합니다. 스타일 차이는 허용하되 팩트는 엄격히 검사합니다.

번역: LLM이 0점에서 5점 척도로 평가 후 정규화합니다. 특히 2점에서 4점 사이 구간의 세밀도를 확장하여 미세한 품질 차이를 학습하도록 유도합니다.

훈련 전략
알고리즘: GRPO 알고리즘을 채택하여 질문 하나에 대해 여러 답변 그룹을 생성하고 상대평가하여 정책을 업데이트합니다.

제약 조건: 길이 제한을 초과하거나 JSON 등 요구된 스키마를 위반할 경우 즉시 0점 처리합니다. 이러한 엄격한 규칙을 통해 모델이 정확한 형식을 준수하도록 강제합니다.

3. 실험 결과
문서 파싱 성능
OmniDocBench: 디지털 문서뿐만 아니라 구겨짐, 접힘, 조명 변화가 심한 Wild 환경에서도 최고 성능을 기록했습니다. 전문 OCR 모델 및 대형 VLM 대비 우수한 강건성을 입증했습니다.

DocML: 한국어를 포함한 14개 언어 테스트에서 SOTA를 달성했습니다.

정보 추출 및 VQA
핵심 정보 추출: 신분증, 송장 등 30개 카테고리 테스트 결과 1위를 차지했습니다. 235B 규모의 Qwen 모델이나 Gemini-2.5-Pro 같은 초대형 모델을 능가했습니다.

비디오 자막: 비디오 자막 추출 작업에서도 가장 높은 정확도를 기록했습니다.

OCRBench: 비슷한 규모의 DeepseekOCR보다 월등히 높은 점수를 보였으며 훨씬 큰 Qwen3VL-2B 모델과 대등한 성능을 발휘
