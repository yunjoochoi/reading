대규모 언어 모델(LLM)의 성능은 크기에 비례하지만, 거대한 모델일수록 추론 속도가 느려짐.
Speculative Decoding은 작은 draft 모델이 여러 개의 후보 토큰을 제안하고, 큰 target 모델이 그것들을 병렬 검증하여 속도를 높음(Llama 8B가 후보 토큰 생성하고 더 큰 모델이 후보 토큰 중에 선택)
기존 SD는 target 모델의 확률 정렬(alignment)(draft 모델이 예측한 결과가 target 모델의 생각과 얼마나 비슷하냐)에 기반해 후보 토큰을 엄격히 거부함 → 많은 정확하지만 비정렬된 토큰들이 낭비됨.
심지어 GPT-4o나 인간이 작성한 텍스트조차도 높은 거부율
정확하지만 비정렬된 후보도 수용할 수 있는 새로운 검증 방식은 없을까?
Judge Decoding 방식은 결과적으로 전통적인 Autoregressive (AR) 방식의 한계, 오류 전파(error propagation) 문제를 완화하고 추론 높이는 방식..


<아키텍쳐>
기존 SD 구조
Draft 모델이 M개의 후보 토큰을 생성 → Target 모델이 확률 기반으로 검증
정렬되지 않으면 토큰 거부 → 낭비

단순 확률 기준 말고, contextual correctness(문맥상 맞는지)로 판단
→ 이 구조는 AR에서의 강한 순차 의존성을 부분적으로 병렬화/완화하고, 오류 전파 가능성을 줄이는 효과를 가질 수 있음

1. Judge Head 추가
Target 모델의 마지막 layer embedding 위에 간단한 선형 분류기(f_judge)를 올림.
입력 토큰의 임베딩만으로 해당 토큰이 문맥상 맞는지 판단.

2. Judge 데이터셋 생성
500개 문항에 대해 "정답/오답 쌍" 생성
각 오답에서 실제로 틀린 토큰 위치를 수동 주석 처리

3. 학습 방식
선형 분류기만 학습 (target 모델은 freeze)
Cross-entropy + 가중치 부여 (positive:negative = 약 20:1)

4. 최종 결정 방식
기존 SD와 Judge의 판단 결과를 OR 조건으로 결합 → 둘 중 하나라도 수용하면 채택


<실험 결과>
속도 향상:
Llama 8B/405B 기준 Judge Decoding → 최대 9.7× 가속, 129 tokens/sec 달성
GPT-fast 기준에서 Eagle-2, Medusa 등 기존 최고속 방법보다 우수

정확도 유지:
GSM8K, HumanEval, MMLU, ARC 등 다양한 벤치마크에서 Target 모델의 성능 거의 보존
단순 Top-K 수용 방식은 성능이 급감함

범용성 실험: 코드 관련 문제를 학습에서 제외하고 평가했을 때도 86.6% → 80.4% 유지 (드래프트보다 높음)

<의의>
Speculative Decoding의 구조적 한계를 지적하고, 그 해결책으로 정렬기반 검증 → 의미 기반 검증으로의 전환을 제안
학습된 선형 judge head만으로도 높은 정확도와 추론 속도 향상을 동시에 달성
작은 draft 모델의 품질 향상을 효과적으로 활용할 수 있는 기반 마련

<한계>
수학적 보장 손실: Judge 기반 수용은 확률적 정렬 기준을 벗어나므로, 표본 분포 보존 보장이 어려움
도메인 일반화 한계: Judge head는 학습된 도메인 밖의 문제에선 성능 저하 가능성 있음(파인튜닝 들어가서)
