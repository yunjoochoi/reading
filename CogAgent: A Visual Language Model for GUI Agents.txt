thus limiting their potential to increase automation levels

utilizing both
low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120×1120, enabling
it to recognize tiny page elements and text.

five text-rich and four general VQA benchmarks,
including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA,
infoVQA, DocVQA, MM-Vet, and POPE

CogAgent는 스크린샷만 입력으로 사용하며, PC와 Android GUI 탐색 작업 모두에서 추출된 HTML 텍스트를 사용하는 LLM 기반 방식(Mind2Web 및 AITW)보다 뛰어난 성능


However, the potential of purely language-based agents is quite limited in realworld scenarios, as most applications interact with humans through Graphical User Interfaces 

construct a large-scale annotated dataset
about GUIs and OCR for continual pre-training

In GUIs, tiny icons
and text are ubiquitous
increasing the resolution of input images results
in significantly long sequence length in language models.
To address this, we design a cross-attention branch
that allows for a trade-off between the resolution and
the hidden size within a proper computation budget


VLM에서 high-resolution image dealing하기= 챌린징
Qwen-VL [2] proposes a position-aware visionlanguage adapter to compress image features, but only reduces sequence length by four and has a maximum resolution of 448 × 448
. Kosmos-2.5 [22] adopts a Perceiver
Resampler module to reduce the length of the image sequence.

결국 LLM에서 포겟팅 문제만 해결되면, VLM에서의 해상도 이해 문제나 기타등등 다 해결된다는거네
롱 시퀀스 핸들하는 문제. 그나마 언어는 지역적 연결이 더 뚜렸하지만 이미지 경우 패치로가면 전역적인 패턴이해가 힘든 문제가 잇음..
여기선 새로운 고해상도 크로스모듈 제안


224×224 이미지는 물체·배치 표현엔 충분하지만 텍스트는 흐릿해지므로, GUI 인식에서는 텍스트 특징을 잘 뽑아내는 고해상도 모듈이 필요하다

일반 도메인에서 학습된 **사전학습 비전-언어 모델(VLMs)**은 보통 매우 큰 **히든 사이즈가 필요합니다.
예: PALI-X, CogVLM → 4,096, LLaVA → 5,120
반면, 문서 OCR처럼 텍스트 중심 태스크에 맞춘 VLM은 더 작은 히든 사이즈로도 충분한 성능을 냅니다.
예: Kosmos-2.5, Pix2Struct → 1,536
텍스트 관련 특징은 상대적으로 작은 히든 차원에서도 효과적으로 포착될 수 있다는 점을 시사


2.3 사전학습
다양한 크기·방향·폰트의 텍스트 인식 능력
이미지 내 텍스트와 객체의 grounding 능력
웹페이지 같은 GUI 이미지를 전문적으로 이해하는 능력

커리큘럼 러닝 적용:
먼저 쉬운 태스크 (합성 텍스트 인식 + 자연 이미지 OCR, 이미지 캡션)
이후 어려운 태스크 (학술 문서 텍스트, grounding 데이터, 웹페이지 데이터) 순차적으로 투입
이렇게 하면 수렴이 빠르고 학습 안정성이 높아짐

2.4 멀티태스크 파인튜닝 및 정렬
수작업 데이터 수집
모바일·PC에서 2천 장 이상의 스크린샷을 수집.
사람 주석자가 각 스크린샷에 대해 화면 요소, 가능한 태스크, 조작 방법을 Q&A 형식으로 주석.


외부 데이터셋 활용
Mind2Web [10], AITW [31]: 웹 및 안드로이드 행동을 다루는 데이터셋 → 태스크, 액션 시퀀스, 스크린샷 포함.
이를 GPT-4를 이용해 자연어 Q&A 포맷으로 변환.

VQA 데이터셋 통합
공개된 여러 비주얼 질문응답(VQA) 데이터셋도 포함.
다양한 태스크를 다루도록 alignment 데이터셋 확장.

학습 설정
이 단계에서는 모델의 모든 파라미터를 unfreeze.
학습 횟수: 10,000 iteration
Batch size: 1,024
Learning rate: 2e-5
