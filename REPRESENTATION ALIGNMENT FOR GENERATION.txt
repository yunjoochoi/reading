REPRESENTATION ALIGNMENT FOR GENERATION:
TRAINING DIFFUSION TRANSFORMERS
IS EASIER THAN YOU THINK

디퓨전은 사실 생성 외에도 표현 학습기이다.
이 모델들이 은닉 상태(hidden state) 안에서 판별적인(discriminative) 특징을 학습한다는 것을 보여줌.
더 성능이 좋은 디퓨전 모델일수록 더 나은 표현을 학습한다는 결과도 보고..
디퓨전 모델의 U-Net 또는 Transformer는 노이즈를 예측하기 위해 학습되지만, 그 과정에서 유용한 표현(representation)도 함께학습. 학습-노이즈를 잘 예측하려면, 이미지의 구조와 의미를 잘 이해해야하기 때문.
초기의 자기지도 학습 방식인 denoising score matching 기법(Vincent, 2011)과도 밀접하게 관련
노이즈가 추가된 입력으로부터 원래 입력 𝑥를 복원하는 작업을 통해, 노이즈 제거 오토인코더의 은닉 상태를 표현으로 간접적으로 학습하는 방식.
이러한 복원(reconstruction) 과제는 좋은 표현을 학습하기 위한 적절한 작업이 아닐 수 있다. =>복원 과정이 표현 학습에 필요 없는 불필요한 세부 정보들까지 보존하려고 하기 때문


최근의 자기지도 시각 표현(Self-supervised visual representations) 기법들을 외부 표현 𝑦로 활용하는 간단한 정규화 기법을 제안하며, 이를 통해 디퓨전 트랜스포머의 학습 효율성과 생성 품질 모두에서 큰 향상을 달성
자기지도 학습 기반 시각 인코더(예: DINOv2, SimCLR, MoCo 등)의 학습 목표와 아키텍처 설계 목적이 생성 모델의 요구와 다르다.
사전 학습된 디퓨전 모델이 실제로 의미 있는 판별적 표현을 학습한다는 점을 선형 프로빙(linear probing) 결과(그림 2a)를 통해 먼저 관찰. 그러나 이 표현들은 DINOv2가 생성하는 표현에 비해 명백히 열등하다.
다음으로, 디퓨전 모델이 학습한 표현과 DINOv2의 표현 사이의 정렬 수준이 아직 약하다는 점도 발견, 이 표현 정렬 정도를 Huh et al. (2024)의 방법에 따라 정량적으로 측정. 마지막으로, 우리는 디퓨전 모델의 규모가 커지고 학습이 오래 지속될수록 DINOv2와의 표현 정렬이 꾸준히 향상된다는 점을 관찰
이러한 통찰들은 우리로 하여금, 외부 자기지도 표현을 통합함으로써 생성 모델을 향상시킬 수 있다는 확신을 갖게 만든다. 하지만 기성(self-supervised) 시각 인코더를 그대로 활용할 경우, 이를 직접 생성 과제에 적용하기는 간단하지 않다. 재구성이나 생성에 적합하게 설계되어 있지도 않지만,
입력 형태 불일치 문제다. 디퓨전 모델은 노이즈가 섞인 입력을 처리하지만, 대부분 자기지도 인코더는 깨끗한 이미지 x를 기준으로 훈련됨. 게다가 잠재 디퓨전 모델의 경우 입력이 사전 학습된 인코더의 출력(잠재 이미지)기 때문에 더욱 문제.
->이러한 기술적 문제들을 극복하기 위해, 우리는 사전 학습된 자기지도 표현을 디퓨전 모델의 표현으로 증류(distillation) 하는 정규화 기법을 통해 디퓨전 모델의 표현 학습을 유도
REPA를 적용하면, 디퓨전 트랜스포머와 DINOv2 간 표현 정렬(alignment)이 크게 향상
몇 개의 레이어 (예: 8개)만 거친 후에도 이 정렬 효과가 뚜렷하게 나타납니다.
***
외부 자기지도 표현을 통합한다는 것은,
DINOv2 같은 **사전학습된 시각 인코더가 만든 좋은 표현(피처 벡터)**을
디퓨전 모델 내부의 표현과 일치하도록(정렬되도록) 학습시켜서
디퓨전 모델이 더 빠르고 똑똑하게 학습되도록 돕는 것



3 REPA: 표현 정렬을 위한 정규화 기법
입력 공간(예: 픽셀)을 예측하는 방식 — 을 통해 좋은 표현을 학습하는 것은 어려울 수 있다.
그 이유는, 모델이 표현 학습에 불필요한 세부 정보까지 그대로 재현하려 하기 때문에,
강력한 표현을 만들기 위해 중요한 '요약 능력'이 떨어질 수 있기 때문이다 (LeCun, 2022; Assran et al., 2023).

우리는 디퓨전 모델을 생성 목적으로 대규모로 학습할 때, 핵심 병목이 바로 이 표현 학습에 있다고 주장
또한, 이러한 표현을 디퓨전 모델이 스스로 완전히 학습하도록 내버려두기보다는,
고품질 외부 시각 표현의 도움을 받아 학습 과정을 더 쉽게 만들 수 있다고 가정

Dit와 Sit를 기반으로 한 REPresentation Alignment (REPA) 라는 간단한 정규화 기법 제안



3.2 관찰 (OBSERVATIONS)
이 문제를 더 깊이 탐구하기 위해, 우리는 먼저 ImageNet (Deng et al., 2009) 데이터셋에서 사전학습된 SiT 모델 (Ma et al., 2024a) 의 레이어별 동작을 조사
특히, 우리는 디퓨전 트랜스포머와 최첨단 자기지도 학습 모델 DINOv2 (Oquab et al., 2024) 사이의 **표현 간 간극(representation gap)**을 측정하는 데 초점을 맞춘다.
------------
선형 프로빙(linear probing)**이란,
어떤 모델의 **은닉 표현(피처)**이 얼마나 잘 구별되는지를 측정하기 위해,
그 표현만 사용해서 아주 단순한 선형 분류기(예: logistic regression)를 학습시켜 보는 것
이미지 x를 디퓨전 모델에 넣어 h 구함
​
 h 를 전혀 fine-tune하지 않고, 위에 선형 분류기만 학습해서 ImageNet 분류를 시도

성능이 높으면 → 그 표현은 좋은 의미 정보를 담고 있다고 판단
-----------
CKNNA: 
CKA (Centered Kernel Alignment) 기반인데,

서로 다른 표현공간을 "가까운 이웃 구조" 기준으로 비교합니다.

이 논문에서는 DINOv2 표현과 SiT 표현이 얼마나 비슷한 구조를 갖는지 측정하는 데 사용
-------------
1. 의미적 간극(semantic gap)
Diffusion transformers exhibit a significant semantic gap from state-of-the-art visual encoders
사전학습된 디퓨전 트랜스포머의 은닉 표현은 layer 20에서 상당히 높은 선형 프로빙 성능을 보이긴 하지만
(Xiang et al., 2023; Chen et al., 2024c와 일치),
DINOv2에 비해서는 성능이 현저히 낮다

2. 특징 정렬의 진행(feature alignment progression)
디퓨전 표현은 다른 시각 표현들과 어느 정도 정렬되어 있다
Figure 2b에서는, SiT와 DINOv2 사이의 표현 정렬도를 CKNNA로 측정한 결과를 제시한다.

흥미롭게도, SiT의 표현은 MAE(He et al., 2022) 보다 DINOv2와 더 나은 정렬을 보인다.
(MAE 역시 마스킹된 패치를 복원하는 자기지도 학습 방법)

하지만, 정렬 점수의 절대값은 여전히 낮으며,
예컨대 MoCov3 (Chen et al., 2021)와 DINOv2 사이의 정렬 정도에 비해서는 떨어진다.
→ 디퓨전 트랜스포머도 자기지도 인코더와 일정 부분 정렬되어 있긴 하지만, 그 수준은 아직 약하다.

3. 최종 특징 정렬 상태(final alignment)
모델 크기와 학습 시간이 늘어나면 정렬은 향상된다

3.3 자기지도 표현(Self-supervised representations)과의 표현 정렬 (Representation Alignment)
REPA는 디퓨전 트랜스포머의 은닉 상태(hidden state)를 사전학습된 자기지도 시각 표현(self-supervised visual representations)과 패치 단위로 정렬.
깨끗한 이미지가 시각 인코더 통과해서 나온 패치단위의 표현을 y*라고 한다. 디퓨전은 노이즈 낀 입력을 받아 히든 스테이트에서 나온 ht를 이 표현 공간 크기에 맞춘다(선형 프로젝션으로)
이후 코사인 유사도 등의 유사도 함수를 사용한 정규화 항을 기존 디퓨전 loss에 더함으로써 정규화. REPA 텀도 최소화시키며 외부 사전학습 인코딩 표현의 유사도와 디퓨전 히든 스테이트 표현을 비슷하게 만든다.

디퓨전 모델이 외부 고품질 표현 공간(y*)과 유사한 표현을 학습하도록 유도함

결과적으로,
표현 학습 능력 강화
학습 속도 향상
생성 품질 향상

Dit에서 사용하는 트랜스포머는 트랜스포머의 인코더 부분. 
