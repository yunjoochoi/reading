1. 이전 연구들은 시각적 특징을 워드 임베딩으로부터 바로 예측하게 했지만 , 우리는 시각과 텍스트 특징을 따로 두는게 성능에 더 낫다는 것을 발견
2. 우리는 클래스 이름 임베딩을 배우게 하기 위해 버트 모델을 사용한다.
3. 우리는 교차 언어 단어 임베딩 정렬 모델에서 영감을 받아 이러한 벡터의 고차원성을 처리하기 위한 전략을 제안 - 버트 임베딩의 서브스페이스가 오히려 더 효과적으로 의미적으로 분류됬다


그러나 GloVe와 같은 표준 단어 벡터는 주제 유사도에 크게 영향을 받습니다. 이는 표 1에 나와 있으며, miniImageNet에서 세 가지 예시 대상에 대해 가장 유사한 상위 3개 클래스를 보여줍니다. 예를 들어, 돗단배의 최근접 이웃에는 스노클(snorkel)과 해파리(jellyfish)가 있습니다. 이 단어들은 모두 주제적으로 명확하게 연관되어 있지만, 돗단배은 스노클이나 해파리와 유사하지 않습니다. 이는 소수 학습(few-shot learning)에 문제가 됩니다.
근데 버트 임베딩은 진짜 유사한 단어들끼리 최근접함

주요 기여는 다음과 같습니다. (i) 시각적 프로토타입과 텍스트 기반 프로토타입을 분리하는 메트릭 기반 FSL 모델에 클래스 이름을 통합하는 간단한 모델을 제안합니다. (ii) BERT를 사용하여 클래스 이름 임베딩을 학습하기 위한 여러 전략을 제안하고 평가합니다. (iii) 시각적 프로토타입과 가장 일치하는 임베딩의 부분 공간을 식별하여 BERT 임베딩의 고차원성을 처리하는 전략을 제안합니다.

이전에는 이미지만 이미지 분류에 쓰였지만, 각 임베딩의 문맥 내 단어 임베딩 또한 이미지 분류에 사용함(버트가틍ㄴ 경우 비슷한 단어 클래스가 이웃에 있으므로 도움 될 것이란 가정에서 시작.)

클래스 이름(BERT 등)으로부터 얻은 정보도 활용해서 분류 성능을 높임.
