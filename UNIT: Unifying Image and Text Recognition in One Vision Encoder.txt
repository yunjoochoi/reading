1. 논문이 나오게 된 배경
기존 Vision Transformer(ViT) 기반 인코더 모델들은 이미지 인식에 뛰어나지만, 텍스트 인식(OCR) 까지 동시에 수행하는 데는 한계= 당연
특히 문서 이미지와 같이 텍스트와 이미지가 혼재된 복잡한 구조를 다루는 데 어려움이 있으며, 
기존 방식은 1. 텍스트와 이미지를 각각 처리 or 2. 멀티모델 조합에 의존하면서 연산량이 커지는 문제
하나의 모델로 이미지와 텍스트 인식을 통합할 수 있는 프레임워크의 필요성 제기

2. 의의
하나의 ViT 기반 모델로 이미지와 텍스트 인식을 통합: 별도 모델 앙상블이나 사전 분류 불필요
추론 비용 증가 없이 성능 확보: 기존 ViT 구조 유지, 추가 컴퓨팅 자원 없이 배포 가능
멀티스케일 훈련을 통한 강건성 확보: 다양한 해상도 문서/이미지에 대응 가능
LVLM 내 통합 가능: Vicuna-7B 기반 LVLM에 통합해 문서 기반 질의응답 성능 향상

3. 한계
전문화된 과업에선 성능 저하 가능
예: Markdown 변환 같은 특수 OCR 과업에서는 Nougat에 비해 성능이 낮음 (Nougat는 해당 과업에 특화되어 학습됨)

텍스트 인식/이미지 인식 간 트레이드오프 가능성
두 능력을 하나의 모델에 담는 과정에서, 극단적인 전문화 과제에는 최적화되지 않을 수 있음

훈련 자원 소모 큼
전체 훈련에 128개의 Ascend 910B GPU, 약 150시간 소요된다는 점에서 높은 리소스를 요구함

Abstract
이미지 인식 과제로 사전학습된 비전 인코더를 기반으로, 텍스트 출력을 위한 경량 언어 디코더+원래 이미지 인코딩 능력의 붕괴(catastrophic forgetting)를 방지하기 위한 경량 비전 디코더
1. Intra-scale Pretraining 단계에서는, 일반적으로 사용되는 해상도의 이미지와 문서를 입력으로 사용하여 통합된 표현(unified representation)을 학습함으로써 기본적인 인식 능력을 갖춥니다.
2. Inter-scale Finetuning 단계에서는 일반적으로 사용되는 해상도와 다른 해상도의 이미지 및 문서 데이터를 활용하여, 모델의 해상도에 대한 강건성(scale robustness)을 향상시킵니다.


INTRO
 문서 분석(document analysis)과 같은 응용에서는, 복잡한 레이아웃(표, 그래프, 혼합 매체 등) 안에 포함된 텍스트 정보를 정확히 인식하고 해석하는 능력이 필수적인데
기존의 비전 인코더 모델들은 인간처럼 이미지와 텍스트를 동시에 인식하는 능력을 보여주지 못한다
ViT(Vision Transformer)는 전역적인 이미지 특징 처리에는 능숙하지만, 특히 고해상도 문서 이미지처럼 세밀한 정보가 중요한 경우, 정확한 텍스트 인식에 필요한 세밀한 국소 정보를 포착하는 데 어려움
1. 가장 직관적인 해결책은, 사전학습된 ViT를 고해상도 문서 데이터로 파인튜닝하는 것입니다. 그러나 이 방식은 **더 긴 시퀀스를 처리하기 위해 사전학습된 위치 임베딩(positional embeddings)을 보간(interpolation)**해야 합니다. 이때 크게 확대된 보간은, 원래 위치 임베딩과 실제 공간 상 위치 간의 정렬(alignment)을 깨뜨리게 되어, 결국 모델 성능에 악영향
2. ViT를 OCR(광학 문자 인식) 과제에 맞춰 전용으로 재학습(retraining)하여 문서 특화 모델(document-specific models)(DONUT)을 만듭니다. 하지만 이 과정에서 ViT가 원래 가지고 있던 이미지 인코딩 능력은 폐기됩니다. 이후 다운스트림 작업에서는 이런 모델들이 다른 전문가 모델들과 앙상블로 사용되며, 사용자가 입력 데이터의 유형(이미지인지 텍스트인지 등)을 사전에 지정해야 하는 불편이 있습니다. 이는 입력 유형에 대한 사전 정보 없이 이미지와 텍스트를 동시에 동적으로 처리해야 하는 시나리오에는 적절하지 않다
3. 앙상블 기반 방법들 [53, 54]은, 이미지 인식과 텍스트 인식을 각각 독립적으로 학습한 동일한 아키텍처의 모델들을 사용하는 방식-계산 비용이 크게 증가
4 .일부 LVLM(mPLUG-Owl2) 기반 방법들 [62, 63]은 문서 분석 과제(예: DocQA)를 OCR-free 방식으로 향상시키려 합니다. 이들은 문서-지시문(document-instruction) 데이터로 LLM을 파인튜닝하지만, 이러한 방식은 텍스트가 뚜렷하게 보이는 이미지에만 잘 작동하며, 텍스트가 조밀하게 배치된 문서에서는 잘 작동하지 않습니다. 또한 다양한 글꼴 크기, 서체, 배경에 일반화가 잘 되지 않는 한계



아키텍쳐
UNIT 아키텍처 개요
이 모델은 고해상도 문서와 저해상도 이미지를 처리하여 시각 토큰(visual tokens) 집합을 생성합니다. 이 토큰들은 입력 임베딩 레이어(input embedding layer)를 거치며, 문서 토큰(document tokens)은 언어 디코더(language decoder)로 전달되어 텍스트 시퀀스를 예측하게 됩니다. 이를 통해 모델의 텍스트 인식 능력을 향상시킵니다.

동시에, 자연 이미지에서 나온 시각 토큰은 경량 비전 디코더(lightweight vision decoder)를 통해 교사 모델(teacher model)의 출력을 모방하며 복원됩니다. 이를 통해 모델이 원래 갖고 있던 이미지 인코딩 능력을 유지하도록 합니다.
추가로, OCR 작업 외에 이미지 캡셔닝(image captioning) 작업도 함께 포함되어 이미지 이해 능력을 한층 더 강화.


UNIT은 하나의 ViT(Vision Transformer) 모델 안에서 이미지 인식과 텍스트 인식 기능을 통합하는 것을 목표로 두 가지 컴포넌트를 추가:

1. 경량 언어 디코더 (예: OPT-125M) 이미지 → 텍스트
문서 수준 OCR을 위한 구성 요소로, ViT로부터 나온 시각 특징을 텍스트 시퀀스로 변환합니다.
ViT 모델이 텍스트를 인식할 수 있는 능력을 갖게 됩니다.

2. 비전 디코더: 유니파이 비전 인코더의 학생 표현을 받아서 원래 ViT가 뽑아내는 좋은 교사 토큰으로 복원하도록(디코딩하도록) 학습됨
ViT 모델이 자연 이미지에 대해 학습했던 기존 시각 인코딩 능력을 보존하기 위해 사용됩니다. = 기존 비전 인코더(vit)능력 유지되게 디코더를 추가함 왜? 문서 인식 (OCR) 학습에 집중하면 기존 이미지 인식 능력이 망가질 수 있음 (→ catastrophic forgetting)
기존 ViT의 출력을 복원하는 방식으로, 각 시각 토큰 단위로 동작합니다 (token-wise feature reconstruction).

또한, OCR 작업 외에도 이미지 캡셔닝 작업을 함께 학습하여 이미지 이해 성능을 강화합니다.
UNIT은 다중 해상도 학습(multi-scale setting) 환경에서 학습되며,
사전 학습 단계(pretraining)에서는 이미지와 문서를 각각 자주 사용하는 해상도로 학습하고
후속 파인튜닝(finetuning) 단계에서는 해상도를 교환해(scale-exchanged) 학습하여 해상도 강건성(scale robustness)을 얻습니다

3.2 Text Recognition Ability Enhancement
Multi-Scale Inputs
ViT는 전역 특징 추출에 탁월하지만,
텍스트 인식에는 적합하지 않은 경우 ViT가 이미지의 각 패치에 대해 **학습된 위치 임베딩(learned position embedding)**을 사용하기 때문입니다.
이 임베딩은 모델이 항상 고정된 해상도 (보통 224, 256, 336 등)에서만 작동
CPE는 원래의 위치 임베딩을 **보간(interpolate)**하여
최대 입력 크기의 포지션 개수에 맞춰 조정합니다.
이후, 저해상도 이미지에 대해선:
위치 임베딩을 무작위로 크롭하고
원래 모델의 입력 패치 수에 맞게 다시 보간

Language Decoder
전통적으로는 CLIP-style의 contrastive learning 방식을 사용해, 이미지와 언어 주석(annotation)을 정렬합니다.
하지만 이 방식에는 다음과 같은 두 가지 주요 단점이 있습니다:

CLIP 텍스트 인코더의 최대 시퀀스 길이가 77 토큰으로 제한됨 →
→ 텍스트가 밀집된 문서(dense documents)에서는 정보가 잘려 나갈 수 있음

contrastive learning은 단어 수준의 정밀한 디코딩에는 적합하지 않음 →
→ 단어별 디코딩이 필요한 OCR 같은 태스크에서 정보 손실 발생
