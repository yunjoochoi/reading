vae 요약하면 기존 오토인코더의 z의 확률분포를 잘 align 시키는거가 끝이다. (loss를 수정해서 가능하게 만듬)
기존 오토인코더는 잠재 벡터 z가 어떤 분포 구조도 가지지 않아서 새로운 샘플 생성도 불가능하다. 잠재벡터에 의미있는 구조가 없기 때문읻.
이를 위해 정규확률분포를 따르는 e를 곱해주는 것을 이용한다. 인코더에서 나온 히든벡터인 h를 리니어 프로젝션 후 각각 mean, std 벡터로 사용함. 이때 std에 e를 곱해서 특정 분포를 따르도록 강제함. = reparameterization(인코더에서 나온 결과를 곱해서, 미분가능하게(tractable하게))
이후 효과적인 loss 모델링을 위해, 실제 사후 분포 p(z|x)와 p를 근사할 특정 분포를 따르는 타겟 분포 q(z) 두 확률분포의 차이를 나타내는 kld를 사용한다. p(z|x)와 q(z)의 중복 제거한 전체 분포와 q 단독 엔트로피의 차이가 최소화되도록, 즉 두 분포가 최대한 같아지게 만드는 것이다. (kld는 비대칭이며, 0 이상)
kld를 전개하면 상수인 log(p(x))가 나오는데, log(p(x))에 대해서 식을 다시 표현하면 kld 텀과 elbo라고 불리는 나머지 텀으로 나눌 수 있다.
목적은 kld를 최소화하여 타겟 확률분포를 근사하는 것이므로 elbo를 최대화하면 kld는 자동으로 최소화되는 식으로 학습이 된다.  
실제 kld는 p(z∣x)를 알 수 없기에 직접적으로 감소시킬수 업어서 elbo를 사용하게 된다.
실제 코드에선 -elbo(loss func)를 최소화하도록 학습된다.

p(z∣x): 진짜 후방 분포 (true posterior) — 이상적이지만 계산이 어려움
q(z∣x): 근사 posterior — 인코더로 학습해서 만드는 것
posterior= 사후확률= 조건부확률
변분추론(Variational Inference)란 사후확률(posterior) 분포 p(z|x)를 다루기 쉬운 확률분포 q(z)로 근사(approximation)하는 걸 말합



VQ VAE는 (1) VAE(표준 정규분포로 고정된 분포)처럼 사전분포를 픽스해놓지 않는다. 대신 학습가능한 상태로 둔다.
* Prior: 표준 정규분포 
Posterior (approx.): 인코더 네트워크에서 입력 x로부터 평균과 분산을 예측하는 학습가능한 분포
이러한 구조에서 ELBO를 최대화하면, posterior 𝑞(z∣x)가 prior p(z)에 가까워지도록 유도

(2) 또 인코더가 연속값이 아닌 코드(이산)을 출력하도록 함
latent z를 codebook에서 벡터를 골라 할당하는 방식으로 모델링하는데, codebook 자체는 학습 가능한 파라미터이니 latent 구조 자체를 학습을 통해 정함

차이점은 VAE 나왔을 시기에는 인코더나 디코더가 그리 강혁하지 않았느데 현재 자기회귀적인 인코더 디코더가 성능이 매우 파워풀함 latent를 정규로 맞추는 작업보다, 포스터리어 칼럽스 문제를 줄여 강력한 자귀회귀 인코더 디코더 사용하는게 성능에 더 좋음(이전 출력을 조건으로 현재 출력을 예측하므로, 정보가 latent 없이도 디코더 내부에서 충분히 흐를 수 있음 → latent가 무시됨)
포스터리어 칼럽스 해결하여면 코드북 필요
=초기 VAE는 단순한 구조의 인코더/디코더를 사용했기 때문에 latent representation을 정규분포로 가정하는 게 합리적이었고, ELBO 구조로도 잘 작동했다. 하지만 modern autoregressive decoder (예: Transformer, PixelCNN)가 너무 강력해지면서, latent가 학습 과정에서 무시되는 posterior collapse 문제가 심각해졌다. 이 문제를 해결하기 위해 VQ-VAE처럼 discrete codebook 기반 latent를 사용하고, 필요하다면 prior도 PixelCNN 같은 autoregressive 모델로 학습시켜 구조적으로 latent를 강제하는 방식이 등장했다.

VQ VAE는 posterior collapse (강력한 디코더 때문에 latent z가 무시됨) 문제를 회피한다. 
오토회귀 사전(prior)와 결합해서 고품질 생성이 가능하게 한다. 

표현을 continuous features로서 배우는게 도미넌트 했지만 discrete representations 배우는 것에 집중한다. 왜냐면 potentially
a more natural fit for many of the modalities (특히 언어는 그 자체로 이산적이다, 스피치도 심볼들의 연속이다... 이미지도 언어로 설명될수 있다)
이전까지는 데ㅣ터를 이산적으로 표현하는데 어려움이 있었지만, 강력한 AR모델이 개발되며 이산 변수를 모델링하는게 가능해졌다. 
vae와 차이점은 정규분포 + KL divergence 대신 → 벡터 양자화 + 코드북을 사용
