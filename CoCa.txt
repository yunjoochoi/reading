CoCa: Contrastive Captioners are Image-Text Foundation Models


컴퓨터 비전에서 사전학습된 큰 모델을 탐색하는것은 주요이슈이다. 왜냐면 이러한 모델들은 많은 다운스트림 태스크에 전이될 수 있다. 
이 페이퍼에서는 coca를 제시하는데, 미니멀리스트 디자인으로, 이미지-텍스트 인코더-디콬더 파운데이션 모델을 contrastive loss, captioning loss와 결합하여 사전학습한다. 그럼으로써 클립의 대조적 접근 방식과simVLM의 생성형 방식을 통합한다. 
일반적인 인코더-디코더 트랜스포머들관 달리(모든 디코더 레이어가 인코더 아웃풋에 의존), CoCa는 디코더 계층 전반부의 교차 어텐션을 생략하여 유니모달 텍스트 표현을 인코딩하고, 나머지 디코더 계층은 멀티모달 이미지-텍스트 표현을 위해 이미지 인코더에 교차 어텐션을 수행
=>유니모달 이미지와 텍스트 임베딩 간에 대조 손실을 적용하고, 멀티모달 디코더 출력에 캡션 손실을 추가하여 텍스트 토큰을 자기회귀적으로 예측, 동일 계산 그래프 사용으로 두 학습 목표는 최소한의 오버헤드로 효율적인 계산.
CoCa는 모든 레이블을 텍스트로 처리하여 웹 스케일 대체 텍스트 데이터와 주석이 달린 이미지 모두에서 처음부터 종단 간 사전 학습을 수행하며, 표현 학습을 위한 자연어 감독을 원활하게 통합합니다. 경험적으로, CoCa는 시각 인식(ImageNet, Kinetics400/600/700, Moments-in-Time), 크로스모달 검색(MSCOCO, Flickr30K, MSR-VTT), 멀티모달 이해(VQA, SNLI-VE, NLVR2), 이미지 캡션(MSCOCO, NoCaps) 등 광범위한 다운스트림 작업에서 제로샷 전이 또는 최소한의 작업별 적응을 통해 최고 수준의 성능을 달성합니다. 특히 ImageNet 분류에서 CoCa는 86.3%의 제로샷 상위 1 정확도, 동결 인코더와 학습된 분류 헤드를 사용할 경우 90.6%, 그리고 미세 조정된 인코더를 사용할 경우 ImageNet에서 91.0%의 최신 상위 1 정확도를 달성


<이전 방법론의 한계>
지피티3이나 버트같은 결루 웹스케일 데이터로 학습되고 일반적인 멀티태스킹 능엵을 제로샷, 퓨샷이나, 전이학습으로 보여줌.
특화된 개별 모델과 비교할 때, 대규모 다운스트림 작업을 위한 기초 모델을 사전 학습하면 학습 비용을 상각할 수 있으며, 인간 수준의 지능을 위해 모델 규모의 한계를 뛰어넘을 수 있는 기회를 제공.
(1) 이미지 분류 데이터 세트에서 교차 엔트로피 손실을 사용하여 사전 학습된 단일 인코더 모델(Lesnet등)의 경우, 레이블이 지정된 벡터인 이미지 주석에 크게 의존하며, 자유형 인간 자연어에 대한 지식을 구축하지 못하여 시각과 언어 양식을 모두 포함하는 작업의 경우 적용이 어렵다.
(2) 웹 스케일 노이즈가 있는 이미지-텍스트 쌍에 대해 대조 손실을 적용한 두 개의 병렬 인코더를 사전 학습하여 이미지-텍스트 기반 모델 만듬(Clip)- 이러한 모델은 융합된 이미지 및 텍스트 표현을 학습할 관절 구성 요소가 없기 때문에 시각적 질의응답(VQA)과 같은 관절 시각-언어 이해 작업에 직접 적용할 수 없다
(3) 인코더-디코더 모델을 이용한 생성적 사전 학습을 통해 일반 시각 및 다중 모달 표현을 학습. (Flamingo)
사전학습 과정에서 모델은 인코더 쪽에서 이미지 수집, 디코더 출력에 LM손실 적용 - 이미지 임베딩에 맞추어 정렬된 텍스트 전용 표현 생성 못하여 멀티모달 정렬 작업에서는 구현가능성, 효율 저하

=본연구에선 단일 인코더, 이중 인코더, 인코더-디코더 패러다임을 통합하고 세 가지 접근 방식의 기능을 모두 포함하는 하나의 이미지-텍스트 기반 모델을 학습


<텍스트-only 디코딩과 멀티모달 디코딩을 함께 수행하는 하이브리드 pretraining학습법>
coca는 엔드 투엔드 학습방식을 따른다. 디코더 트랜스포머를 텍스트 유니모달, 멀티모달 디코더로 분리 

CaCa는 이미자-텍스트 페어 사용하여 학습.
1. contrastive alignment와
2. 텍스트 생성
을 end to end(인코더-디코더까지) 학습하는 모델이다.

# 비전 인코더: 일반적인 ViT기반 인코더, 이미지를 토큰 단위로 인코딩

# 디코더 트랜스포머를 텍스트 유니모달, 멀티모달 디코더로 분리
Text-only Decoder: LM loss 사용하여 언어 모델처럼 학습
Multimodal Decoder: PrefixLM loss 사용하고, 이미지 설명 캡션 생성
=>학습 시 두 디코더 모두 업데이트되어 joint training

=> 세개의 loss를 동시에 최적화
Contrastive Loss	이미지-텍스트 간 의미 정렬 (CLIP과 동일하게)
Captioning Loss (PrefixLM)	이미지 + 텍스트로 캡션 생성 (multimodal decoder)
Language Modeling Loss (LM)	텍스트만으로 다음 토큰 생성 (text-only decoder)

디코더 나눈 이유
텍스트-only 디코더는 언어 자체에 대한 이해를 강화하고,
멀티모달 디코더는 이미지 조건으로 자연어 생성

*LM Loss (Language Modeling Loss): GPT처럼 입력된 텍스트의 앞부분을 보고 다음 토큰을 예측
*PrefixLM Loss: 이미지 + 텍스트의 일부(prefix)를 조건으로 다음 텍스트 토큰을 생성
입력: [image tokens] + "A dog is"
출력: "running in the park"

즉, 디코더는 "A dog is" 이후의 텍스트 continuation을 생성하도록 학습


<pretraining 이후, 다양한 다운스트림 멀티모달 태스크에 파인튜닝(fine-tuning)>
CoCa는 unified 학습된 구조이기 때문에, 사전학습만으로도 매우 강력한 성능
Zero-shot / Few-shot Inference: 별도 추가 학습 없이 즉시 여러 태스크에 사용 가능. 이미지와 질문을 넣으면 바로 답을 생성할 수 있으며 자연어 프롬프트 사용 가능..

<성능>
| 태스크                      | 벤치마크                 | 성능                           |
| ------------------------ | -------------------- | ---------------------------- |
| Image Captioning         | NoCaps, COCO         | SOTA (CIDEr, BLEU 등에서 최고 수준) |
| VQA                      | VQAv2                | 기존 모델 대비 향상                  |
| Image-Text Retrieval     | MS-COCO, Flickr30k   | CLIP보다 향상된 R\@1              |
| Zero-shot Classification | ImageNet, ImageNet-R | CLIP 대비 더 강력                 |
