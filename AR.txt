이 [seq_len x 512] 벡터들을 "다시 토큰 아이디"로 바꾸려면,
이 벡터가 원래 토큰 임베딩 공간에 존재해야 하지 않나?
근데 Transformer는 context-aware representation이라 그 벡터가 토큰 임베딩이랑 일치하지 않을 수도 있지 않나?
맞음
보통 아래와 같은 방식으로 가장 가까운 토큰을 찾는다:

방식: nearest neighbor (argmax)
토큰 임베딩 테이블과 내적 → 가장 유사한 토큰 선택
token_id = argmax(dot(output_vec, token_embedding_matrix.T))

Decoder에서는? vocab 확률 분포를 만들기 위해 토큰 임베딩 weight를 transpose해서 softmax로 예측
logits = transformer_output @ embedding_matrix.T
시퀀스 전체를 한꺼번에 예측하는 구조
transformer_output.shape == (seq_len, hidden_dim)       # 예: (128, 512)
embedding_matrix.shape == (vocab_size, hidden_dim)       # 예: (30522, 512)

logits = transformer_output @ embedding_matrix.T         # (128, 30522)

정리
디코더에서의 토큰 예측이든,
인코더에서의 마스크 복원이든,
결국은 어떤 토큰 벡터가 가장 유사한가?를 묻는 문제고,
그 유사도는 사실상 cosine similarity처럼 작동하는 dot product로 계산


=============================================================================



개념적으로도 AR 모델은 KV 캐시 덕분에 추론 시 매 스텝마다 토큰 하나만 처리하면 되므로, 전체 토큰을 매번 계산해야 하는 diffusion보다 연산 효율이 더 높을 수 있음.
AR 모델의 실용성 강조
실제로는 대부분의 모델이 다운스트림 작업에 대해 supervised fine-tuning을 거친 후 사용되므로, AR 구조는 다음과 같은 장점을 가지게 됨:

임의 길이 시퀀스 생성 가능

멀티태스크/멀티모달 추론

이미지/텍스트 인페인팅 또는 편집 용이

반면 diffusion 모델은 토큰 길이 제한이 있어 동적 시퀀스 길이 작업에 어려움이 있으며, VQA나 멀티턴 대화 모델로 튜닝하기 매우 비유연함.
Fig.3을 보면 제안 방식이 AR 방식보다 효율적이지 않은 것으로 보임.


기존 LLM이 역방향 시퀀스 생성에 취약한 반면, LLaDA는 reversal curse 문제를 효과적으로 해결
