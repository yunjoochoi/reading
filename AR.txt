이 [seq_len x 512] 벡터들을 "다시 토큰 아이디"로 바꾸려면,
이 벡터가 원래 토큰 임베딩 공간에 존재해야 하지 않나?
근데 Transformer는 context-aware representation이라 그 벡터가 토큰 임베딩이랑 일치하지 않을 수도 있지 않나?
맞음
보통 아래와 같은 방식으로 가장 가까운 토큰을 찾는다:

방식: nearest neighbor (argmax)
토큰 임베딩 테이블과 내적 → 가장 유사한 토큰 선택
token_id = argmax(dot(output_vec, token_embedding_matrix.T))

Decoder에서는? vocab 확률 분포를 만들기 위해 토큰 임베딩 weight를 transpose해서 softmax로 예측
logits = transformer_output @ embedding_matrix.T
시퀀스 전체를 한꺼번에 예측하는 구조
transformer_output.shape == (seq_len, hidden_dim)       # 예: (128, 512)
embedding_matrix.shape == (vocab_size, hidden_dim)       # 예: (30522, 512)

logits = transformer_output @ embedding_matrix.T         # (128, 30522)

정리
디코더에서의 토큰 예측이든,
인코더에서의 마스크 복원이든,
결국은 어떤 토큰 벡터가 가장 유사한가?를 묻는 문제고,
그 유사도는 사실상 cosine similarity처럼 작동하는 dot product로 계산


=============================================================================



개념적으로도 AR 모델은 KV 캐시 덕분에 추론 시 매 스텝마다 토큰 하나만 처리하면 되므로, 전체 토큰을 매번 계산해야 하는 diffusion보다 연산 효율이 더 높을 수 있음.
AR 모델의 실용성 강조
실제로는 대부분의 모델이 다운스트림 작업에 대해 supervised fine-tuning을 거친 후 사용되므로, AR 구조는 다음과 같은 장점을 가지게 됨:

임의 길이 시퀀스 생성 가능

멀티태스크/멀티모달 추론

이미지/텍스트 인페인팅 또는 편집 용이

반면 diffusion 모델은 토큰 길이 제한이 있어 동적 시퀀스 길이 작업에 어려움이 있으며, VQA나 멀티턴 대화 모델로 튜닝하기 매우 비유연함.
Fig.3을 보면 제안 방식이 AR 방식보다 효율적이지 않은 것으로 보임.


기존 LLM이 역방향 시퀀스 생성에 취약한 반면, LLaDA는 reversal curse 문제를 효과적으로 해결

============================================================================
AR 모델이 순차적 의존성을 포착할 수 있게 하는 바로 그 메커니즘(예: 높은 지연 상관관계 )이 역설적으로 오류 증폭을 직접적으로 야기

디퓨전 모델은 제어 가능함
classifier guidance: 초기 접근 방식 중 하나, 외부 분류기 사용하여 디퓨전 과정을 원하는 클래스로 유도
CFG: 샘플링과정에서 조건부 모델 예측과 무조건부 모델 예측 사이 보간 (장점-사전 훈련된 모델에 아키텍처 변경 없이 적용할 수 있고, 추론 시 유연하게 조절 가능)
단점-각 단계에서 두 번의 순방향 패스가 필요하므로 계산 비용이 일반적으로 두 배로 증가

기타방법: 보상 유도 생성(reward-guided generation), 순차적 몬테카를로(SMC) 기반 안내, 가치 기반 샘플링(value-based sampling)

디퓨전 모델의 반복적 특성은 본질적으로 생성 전반에 걸쳐 일종의 경로 수정을 허용(노이즈 없애는 과정)(각 노이즈 제거 단계에서 생성물은 조건부 정보를 향해 조금씩 밀어짐)
경로에 한번 전념하면 벗어나기 어려운 AR 모델의 경향과 대조

===
need some way to teach models to work with concepts first, and grammar second. Let's combine Meta's Large Concept Models and Diffusion Language models to achieve Diffusion Concept Models
====
난 생각할 때 대충 큰 그림부터 잡고 점점 구체화시키는데, 이게 디퓨전 방식과 비슷해.
근데 디퓨전은 앞에서 했던 생각을 기억 못해서, 사람이 하는 것처럼 되돌아가는 게 어렵지 않을까?
그래서 오토리그레션처럼 순차적으로 생각을 만든 다음, 디퓨전처럼 다듬으면 좋지 않을까?
그리고 우리는 종종 거꾸로 생각하니까 ‘뒤에서 앞으로 예측하는’ 오토리그레션도 가능하지 않을까?”
====
디퓨전 활용한 보간 방식 -언어 생성/이미지 생성
언어 생성에는 블록디퓨전(인데 마스크방식말고 리얼 토큰임베딩에 노이즈입히는방식)/ 이미지 생성에는 hart방법론
둘 다 ar로 초안 짜고 디퓨전으로 다듬는 방법


하지만 디퓨전은 "과거를 지운다"는 단점이 있다.(이는 구간당 오토리그레시브 써서 보간)
그래서 디퓨전 + 오토리그레션을 결합해서 써보면 어떨까 생각했다.

예: 생각을 오토리그레시브하게(순차적으로) 생성한 후,

그걸 디퓨전으로 다시 다듬는다는 식.

오토리그레션은 시간적 흐름을 잘 잡고, 디퓨전은 정제/전체 조율에 강함, 역방향 오토리그레션도 흥미롭다고 느꼈다.
==================

통합 관점: "AR 초안 + 세분화 디퓨전 보정" 이중 단계 프레임워크
두 방식은 사실 다음과 같은 공통 구조를 갖습니다:

초안 생성 단계:

Autoregressive 모델로 coarse sequence (이미지 코드북 or 텍스트 블록) 생성

후처리 정제 단계:

디퓨전 방식으로 세밀한 복원

이 때 복원의 대상은:

HART: continuous latent residual (fine pixel/codebook 차이)

Block Diffusion: discrete masked tokens (fine word refinement)

즉, 이 구조는 **"AR로 high-level structure → Diffusion으로 low-level refinement"**라는 계층적 생성 전략
<발전 방향>
modality-agnostic한 unified 구조 구현 → 이미지, 텍스트, 오디오 모두에 사용 가능

Diffusion 단계의 조건부 강도를 조절해 생성 다양성 vs 정확도 트레이드오프 제어

block 단위 조절을 통해 계층적 reasoning에도 응용 가능 (특히 AGI 문맥에서 유용)



