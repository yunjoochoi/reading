# CoT
지피티3 파인튜닝한것보다 성능 좋고, sota.
모델 스케일링 하는 방법으로는 수학, 일반상식, 상징추론등의 능력이 향상되는데 한계
#cot로 모델 성능 향상되는 이유
1. 수학적 추론은 자연어 근거를 생성하는것에서 도움을 받을 수 있다.
2. llm은 프롬프팅을 통해 인컨텍스트, 퓨숏러닝 가능하다. - 비용 많이 드는 파인튜닝이 필요없다
(파인튜닝은 태스크 별로 학습을 진행해야함, 제너럴리티를 잃음)

# 근거 프롬프팅의 문제점
=>큰 사이즈의 좋은 퀄리티 근거 생성의 비용
(그냥 퓨샷으로는 리즈닝에 poor하게 작동=>트리플 넘겨서 프롬프팅<input, chain of thought, output>)

# 방법론: 사람이 수학 문제 출떄 추론하는 방식을 그대로 자연어로 표현하여 퓨숏샘플로 프롬프팅한다.

# Chain-of-thought 장점
1. 다단계 문제를 중간 단계로 분해하고, 푸는데 여러 추론을 거쳐야 하는 문제엔 추가 계산 할당 가능
2. 모델 동작의 해석가능성, 디버깅 가능 (하지만 모델이 실제 사람처럼 리즈닝하는지 보장은 못함)
3. 인간이 언어 통해 해결가능한 모든 문제에 적용가능
4. 기성언어모델에 그냥 프롬프트 추가해서 쉽게 활용 가능



<느낀점>
양질의 데이터만 있으면, 텍스트 도메인 only로는 현재기술로도 매우 큰 발전가능할듯
결국 데이터의 문제이고 방법론으로의 sota가 가능할까?
즉, 이제 기술의 문제는 멀티모달에 있는듯