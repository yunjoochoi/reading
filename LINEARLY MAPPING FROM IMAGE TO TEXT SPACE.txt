선형 변환만으로도 강력한 성능을 보이지만, 이러한 모델들의 표현 공간(representation space)에는 언어 공간으로는 근사할 수 없는 차이점들이 존재하는 것으로 보인다. 이러한 차이점을 잘 활용하면, 멀티모달 모델은 더 풍부한 표현을 학습할 수 있다. 현재의 멀티모달 사전학습 목적(objective)이 이러한 차이점을 어떻게 잘 활용하거나 활용하지 못하는지를 고민하는 것이 유용하다.

LiMBeR는 **비전-언어 간 최소한의 매핑(minimal mapping)**을 제공하는 비교 기준점으로서, 향후 멀티모달 모델의 강력한 베이스라인 역할을 할 수 있다.

우리는 LiMBeR를, 서로 다른 모달리티에서 학습된 표현들이 얼마나 유사하거나 다른지를 이해할 수 있는 유용한 도구로 본다. 만약 어떤 개념이 서로 비슷하게 표현된다면, 그 유사성을 이용해 좋은 텍스트 표현을 학습하는 데 필요한 데이터 양을 줄일 수 있을까? 반대로 표현이 서로 다르다면, 멀티모달 모델이 두 모달리티에서 오는 정보를 결합해 더 풍부한 표현을 학습할 수 있을까?

예를 들어, 시각 데이터(vision data)는 **텍스트 코퍼스에 존재하는 리포팅 편향(reporting bias)**을 보완하는 데 도움이 될 수 있다 (Paik et al., 2021).

이러한 질문에 답하는 것은, 텍스트 전용 사전학습의 한계를 이해하고, 언어 모델을 비언어적 데이터에 보다 잘 연결(grounding)시키는 방법을 찾는 데 도움이 될 것이다.



본 논문에서는 사전학습된 이미지 표현과 텍스트 표현이 얼마나 유사한지를 평가하기 위해, 이미지 표현을 언어 모델(LM)의 입력 공간으로 선형 변환하는 매핑을 학습합니다. 이렇게 하면 LM이 이미지의 내용을 정확하게 설명할 수 있게 됩니다.

우리는 LiMBeR(Linearly Mapping Between Representation spaces)를 통해 학습된 모델이, 이미지 및 텍스트 네트워크 모두를 튜닝하는 MAGMA와 같은 모델에 비해 이미지 캡셔닝과 시각 질문응답(Visual Question Answering) 벤치마크에서 경쟁력 있는 성능을 보임을 보여줍니다.

하지만 이러한 정보 전이의 성능은 이미지 인코더 백본이 사전학습 중 얼마나 많은 언어적 감독(linguistic supervision)을 받았는지에 크게 좌우된다는 것도 발견했습니다. 예를 들어, 언어 정보 없이 시각 정보만으로 학습된 BEIT는, 이미지 분류로 훈련된 ResNet보다 성능이 낮았고, 이 ResNet조차도 자연어 캡션으로 사전학습된 CLIP보다 성능이 낮았습니다.

우리는 어떤 개념 정보가 성공적으로 전이되는지 탐구했으며, 생성된 텍스트 분석, 클러스터링, 프로빙을 통해 다음과 같은 결론에 도달했습니다:

LM과 비언어적 이미지 표현 간의 표현 공간 유사성은 주로 **지각적 특징(perceptual features)**과 같은 거칠고 일반적인 개념 수준에 머무르며,

언어적 감독을 받은 시각 모델만이 **정확한 어휘 수준의 개념(lexical concepts)**을 전이할 수 있습니다.

이러한 결과는, 언어 모델과 시각 모델이 개념적으로 유사한 표현 공간을 학습하고 있으며, 이로 인해 간단한 선형 변환만으로도 이미지 정보를 언어 모델로 전이하는 데 충분할 수 있음을 시사합니다.

하지만 이러한 표현 공간 간의 유사성이 정확히 어느 정도인지는 아직 명확히 이해되지 않았으며, 이는 향후 연구에서 탐색할 만한 흥미로운 주제
