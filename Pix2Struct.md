# **Pix2Struct**

요약: 

(1) 새로운 트레이닝 방법 제시: parse masked screenshots of web pages → simplified HTML

웹은 비주얼요소 많음 그래서 데이터 구하기 쉬움, self-supervised pairs로 학습하는데 그냥 웹페이지 캡쳐하고 HTML크롤링하면 됨

(2) variable-resolution input 표현을 배운다

(3) 파인튜닝 단계: 질문 같은 언어 프롬프트를 입력 이미지 위에 직접 렌더링하여 학습

하나의 사전학습 모델이 문서, 삽화, 사용자 인터페이스, 자연 이미지 등 네 가지 도메인에 아홉 가지 작업 중 여섯 가지에서 SOTA

목표:

- **입력**: 오직 원시 픽셀(raw pixels) 형태의 이미지
- **출력**: 토큰 시퀀스로 된 텍스트
- Donut과 유사한 구조지만, 목표는 T5(Raffel et al., 2020) 같은 모델의 시각적 유사체(visual analog)를 만드는 것.
- **더 폭넓은 작업과 도메인**에서 파인튜닝할 수 있는 **단일 사전학습 모델 제시**

2.2. 아키텍처 (Architecture)

표준 ViT는 **고정 크기 패치**를 뽑기 전에 **입력을 사전에 정의된 해상도로 리사이즈**

이 과정에서 두 가지 문제: 

1. 종횡비(aspect ratio) 왜곡
    - 문서, 모바일 UI, 그림 등은 종횡비가 다양하다.
    - 하지만 강제로 리사이즈하면 원본 비율이 깨져 왜곡이 생긴다.
2. 다운스트림 태스크 전이의 어려움
    - 사전학습은 하나의 특정 해상도만 본다.
    - 따라서 더 높은 해상도를 요구하는 새로운 태스크로 옮길 때 비효율적이다 (Touvron et al., 2019; Wang et al., 2021b).

### Pix2Struct의 제안

- 우리는 입력 이미지를 미리 정해진 해상도로 리사이즈하지 않고,
    
    주어진 시퀀스 길이 안에서 최대한 많은 고정 크기 패치를 추출할 수 있도록 스케일링한다.
    
- 그리고 모델이 가변 해상도를 명확히 처리할 수 있도록, 2차원 절대적 위치 임베딩(2D absolute positional embeddings)을 패치 입력에 사용

장점

- 극단적인 종횡비에 대응 가능 (문서, UI 등에서 흔히 나타남)
- 시퀀스 길이·해상도를 즉석(on-the-fly)에서 변경할 수 있음 → 다양한 다운스트림 태스크에 적응 용이

2.3. Pretraining

- Pix2Struct의 사전학습 목적은 **입력 이미지의 내재된 구조(structure)를 표현**할 수 있도록 만드는 것.
- 이를 위해 **웹페이지의 스크린샷 + HTML 소스**를 이용해 **셀프 슈퍼바이즈드 학습 쌍**을 만든다.

- 각 웹페이지에 대해:
    - **HTML 소스**를 수집
    - **1024 × 1024 뷰포트(viewport)** 크기로 **스크린샷** 생성

학습 신호(signal)를 풍부하고 밀도 있게 주기 위해 **스크린샷과 HTML**을 가공:

1. **HTML DOM 트리 압축**
    - **보이는 요소** 또는 그 자손만 유지
    - 만약 어떤 노드가 보이는 요소를 포함하지 않고 자식이 하나뿐이면, 그 자식을 해당 노드와 합쳐서 불필요한 중첩 제거
2. **노드 정보 단순화**
    - 노드에는 **텍스트, 이미지 파일명, alt-text**만 유지
    - 태그, 스타일, title, URL 등은 이번 연구에서는 제외 (추후 연구 가능성)
3. **시퀀스 길이 제약**
    - 미리 정해둔 디코더 시퀀스 길이에 맞도록 **가장 큰 linearized subtree**를 선택
    - 선택된 서브트리가 커버하는 영역을 **bounding box**로 스크린샷에 표시

- BART에서 착안 → 더 나은 컨텍스트 모델링을 위해 **50% 텍스트를 마스킹**
- 마스킹된 영역은 **랜덤으로 선택된 연속된 span**이며, 스크린샷 위에 마스크를 렌더링
- 모델은 **마스킹된 부분을 포함해 subtree 전체를 디코딩**하도록 학습

2.4. Warming up with a reading curriculum

- Pix2Struct를 곧바로 스크린샷 파싱과제로 사전학습할 수는 있지만
    - 학습이 불안정하고, 수렴 속도도 느려진다.
- 따라서 짧은 워밍업 단계를 먼저 두면 효과적이라는 걸 발견
    - 이 단계에서는 모델을 단순히 읽기에 노출
    - 즉, 텍스트 조각(snippet)을 무작위 색상과 폰트로 만든 이미지로 변환하고,
    - 모델은 원래 텍스트를 디코딩하도록 훈련
- 이렇게 하면 커리큘럼 러닝(curriculum learning) 효과가 생겨서:
    1. 사전학습이 더 안정적이고 빠르게 수렴하며,
    2. 파인튜닝 성능도 향상된다 (5장에서 실험 결과로 논의).
- 이 방식은 Dessurt (Davis et al., 2022)에서도 사용되었고, Donut의 사전학습을 단순화한 버전으로도 볼 수 있다.

2.5. Finetuning

Pix2Struct 파인튜닝 = 전처리를 통해 모든 입력을 이미지로 만들고, 출력은 텍스트로 단순화

### 1) 캡셔닝 (Captioning)

- 가장 단순한 경우.
- 입력: 이미지
- 출력: 캡션 텍스트
- 예: TextCaps, Screen2Words
- 만약 캡션이 특정 bounding box를 대상으로 한다면 (Widget Captioning) → 해당 박스를 이미지에 직접 그려서 제공.

### 2) 시각적 질문응답 (Visual Question Answering, VQA)

- 예: OCR-VQA, ChartQA, DocVQA, InfographicsVQA
- 기존 멀티모달 모델: 질문을 별도의 텍스트 채널로 넣음.
- Pix2Struct: 질문을 원본 이미지 상단(header)에 직접 렌더링.
    - 따라서 모델은 이미지와 질문을 모두 시각 모달리티로 읽음.
- 이는 GPT(Radford et al., 2018) 이후 NLP에서 입력들을 단순히 연결 학습하는 전략과 유사.
- 효과적인 이유: Pix2Struct는 이미 입력 이미지 내 장거리 상호작용을 학습했기 때문.
- 선다형 질문 (AI2D) → 보기(choice)도 질문과 함께 헤더에 렌더링.

### 3) 지시 표현 해석 (Referring Expression, RefExp)

- 가장 복잡한 경우.
- 태스크: 자연어 표현이 어떤 UI 컴포넌트를 가리키는지 선택.
- 방법:
    - 후보 UI 컴포넌트마다 학습 인스턴스를 생성
    - 입력 이미지: 후보 컴포넌트의 bounding box + 참조 표현(referring expression)을 포함
    - 출력: `"true"` 또는 `"false"`
    - 학습 시: 양성 후보(positive) 1개당 음성 후보(negative) 5개를 샘플링
    - 추론 시: `"true"`를 가장 높은 확률로 생성한 후보를 선택

느낀점:

보통 가변 해상도 쓰면 2차원 절대적 위치 임베딩 사용하는듯

스크린샷 위에 마스크를 렌더링해서 학습할 수도 있구나(바트/버트 방식이긴함)
