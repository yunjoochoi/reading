Beyond Text: Frozen Large Language Models in Visual Signal Comprehension

대형 언어 모델(LLM)이 멀티모달 데이터셋에 대한 별도의 파인튜닝 없이도 시각 정보를 이해할 수 있는 가능성을 탐구한다. 핵심 아이디어는 이미지를 일종의 언어로 간주하고, 이를 LLM의 어휘로 구성된 이산적 단어 시퀀스로 변환하는 것
이미지 복원 및 노이즈 제거 작업까지 수행 가능해진다. 특히 이 모든 작업은 LLM을 파인튜닝하지 않고도 오토레그레시브 방식으로 이루어짐

인트로
이전 시각 비전 모델들은 추가적인 파인튜닝 -특히 llm을- 필요로 함 
Our technique translates an image into
a collection of discrete tokens that are within the vocabulary of the LLM. Once translated, these tokens can be fed
into the LLM, enabling it to process and comprehend visual
information

Translating an image into a set of tokens that a frozen
LLM can understand is challenging.

a tokenizer designed to map images to the input (token) space of a frozen LLM

LLM의 어휘는 BPE나 SentencePiece 덕분에 단어 및 서브워드 단위로 구성되며, 어휘 크기가 클수록 이미지 표현력이 향상된다.

본 연구는 어휘력을 높이기 위해 기존 어휘를 바이그램·트라이그램 형태로 확장하여 이미지 표현 능력을 강화했다.

V2L Tokenizer는 이미지 패치를 LLM 토큰으로 변환할 뿐 아니라, 전체 이미지를 요약하는 글로벌 표현도 생성한다.

In-context learning은 LLM이 제로샷 추론을 잘 수행하도록 도와주는 효과적인 방법이다.

본 연구는 파인튜닝 없이, 몇 가지 예시를 제시해 LLM이 시각 언어 패턴을 모방하도록 유도한다.

이를 통해 LLM은 이미지에서 변환된 "외국어" 형태의 시각 정보를 더 잘 이해할 수 있게 된다.

 MiniGPT-4 [62] and LLaVA [24]
confirm that tuning a single linear layer on high-quality instruction data, is sufficient for feature alignment.

they lack the ability to generate visual
content and necessitate the collection of additional imagetext pairs to train the vision-language alignment modules.


SPAE, LQAE However, because of the substantial difference between visual features
and language token embeddings, those methods struggle
to assign semantic language tokens to images. This limitation hinders LLMs from fully understanding visual signals within a given context.

3.2. Vision-to-Language Tokenizer
Global Codebook.
These subword elements, in general, tend to have limited semantic
significance. To enhance the semantic representation of
entities within the LLM vocabulary T of size N, we introduce a vocabulary expansion technique.
LLM의 토큰들은 사실 그 자체들로 크게 의미를 지니고 있지 않음.. 엄청 쪼개졌기 때문이다..
However, it is important to note
that the resulting bigrams and trigrams may not necessarily
convey meaningful semantics, Moreover, the generation
of bigrams and trigrams leads to a vast number of possible combinations

we
compute the CLIP similarities [36] between each image in
the dataset and every lexical item in the expanded LLM vocabulary. We then record the top-5 lexical items with the
highest similarity scores for each image. Finally, we aggregate these top-5 lexical items from all images to form the
final expanded LLM vocabulary, which serves as our global
codebook.

Local Codebook.
We use the original LLM
vocabulary as our local codebook


 Additionally, we utilize a trainable projector, which is implemented as a linear
layer, to further project the LLM embeddings for alignment
with the visual space.

img encoder, CNN encoder and a frozen CLIP-vision-encoder. The CNN encoder
is identical to the one used by VQ-GAN ], but with modifications to the downsampling rate. 
The CNN encoder aims to extract
local information, while the CLIP-vision-encoder focuses
on encoding global information.
